% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pmmh.R
\name{pmmh}
\alias{pmmh}
\title{Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models}
\usage{
pmmh(
  y,
  m,
  init_fn,
  transition_fn,
  log_likelihood_fn,
  log_priors,
  init_params,
  burn_in,
  num_chains = 4,
  algorithm = c("SISAR", "SISR", "SIS"),
  resample_fn = c("stratified", "systematic", "multinomial"),
  param_transform = NULL,
  tune_control = default_tune_control(),
  verbose = FALSE,
  return_latent_state_est = FALSE,
  seed = NULL
)
}
\arguments{
\item{y}{A numeric vector of observations.}

\item{m}{An integer specifying the total number of MCMC iterations.}

\item{init_fn}{A function to initialize the state-space model.}

\item{transition_fn}{A function that defines the state transition of the
state-space model.}

\item{log_likelihood_fn}{A function that calculates the log-likelihood
for the state-space model given latent states.}

\item{log_priors}{A list of functions for computing the log-prior of each
parameter.}

\item{init_params}{A vector of initial parameter values.}

\item{burn_in}{An integer indicating the number of initial MCMC iterations
to discard as burn-in.}

\item{num_chains}{An integer specifying the number of PMMH chains to run.}

\item{algorithm}{A character string specifying the particle filtering
algorithm to use. Must be one of \code{"SISAR"}, \code{"SISR"}, or
\code{"SIS"}. Defaults to \code{"SISAR"}.}

\item{resample_fn}{A character string specifying the resampling method.
Must be one of \code{"stratified"}, \code{"systematic"}, or
\code{"multinomial"}. Defaults to \code{"stratified"}.}

\item{param_transform}{An optional character vector that specifies the
transformation applied to each parameter. Currently supports
\code{"log"}, \code{"logit"}, and \code{"identity"}. If \code{NULL}, the
\code{"identity"} transformation is used for all parameters.}

\item{tune_control}{A list generated by \code{\link{default_tune_control}}
containing tuning parameters for the pilot chain, such as \code{pilot_m},
\code{pilot_n}, \code{pilot_reps}, \code{pilot_proposal_sd},
\code{pilot_algorithm}, and \code{pilot_resample_fn}.}

\item{verbose}{A logical value indicating whether to print information about
pilot_run tuning. Defaults to \code{FALSE}.}

\item{return_latent_state_est}{A logical value indicating whether to return
the latent state estimates for each time step. Defaults to \code{FALSE}.}

\item{seed}{An optional integer to set the seed for reproducibility.}
}
\value{
A list containing:
\describe{
  \item{\code{theta_chain}}{A matrix of post burn-in parameter samples.}
  \item{\code{latent_state_chain}}{If \code{return_latent_state_est} is
  \code{TRUE}, a list of matrices containing the latent state estimates
  for each time step.}
  \item{\code{diagnostics}}{Diagnostics containing ESS and Rhat
  for each parameter.}
}
}
\description{
This function implements a Particle Marginal Metropolis-Hastings (PMMH)
algorithm to perform Bayesian inference in state-space models. It first
runs a pilot chain to tune the proposal distribution and the number of
particles for the particle filter, and then runs the main PMMH chain.
}
\details{
The PMMH algorithm is essentially a Metropolis Hastings algorithm
where instead of using the exact likelihood it is estimated using a particle
filter (see also \code{\link{particle_filter}}). This implementation
has two main steps:
\enumerate{
  \item \strong{Pilot Chain:} A pilot particle chain is run using the
  settings provided in \code{tune_control} to obtain initial estimates for
  the parameter vector, its covariance, and the number of particles for
  the Particle Filter.
  \item \strong{Main MCMC Chain:} The main PMMH chain is executed for
  \code{m} iterations using the tuned settings.
}
}
\examples{
init_fn <- function(particles) {
  stats::rnorm(particles, mean = 0, sd = 1)
}
transition_fn <- function(particles, phi, sigma_x) {
  phi * particles + sin(particles) +
    stats::rnorm(length(particles), mean = 0, sd = sigma_x)
}
log_likelihood_fn <- function(y, particles, sigma_y) {
  stats::dnorm(y, mean = particles, sd = sigma_y, log = TRUE)
}
log_prior_phi <- function(phi) {
  stats::dnorm(phi, mean = 0, sd = 1, log = TRUE)
}
log_prior_sigma_x <- function(sigma) {
  stats::dexp(sigma, rate = 1, log = TRUE)
}
log_prior_sigma_y <- function(sigma) {
  stats::dexp(sigma, rate = 1, log = TRUE)
}
log_priors <- list(
  phi = log_prior_phi,
  sigma_x = log_prior_sigma_x,
  sigma_y = log_prior_sigma_y
)
# Generate data
t_val <- 20
x <- numeric(t_val)
y <- numeric(t_val)
x[1] <- rnorm(1, mean = 0, sd = 1)
y[1] <- rnorm(1, mean = x[1], sd = 0.5)
for (t in 2:t_val) {
  x[t] <- 0.8 * x[t - 1] + sin(x[t - 1]) + rnorm(1, mean = 0, sd = 1)
  y[t] <- x[t] + rnorm(1, mean = 0, sd = 0.5)
}
# Should use much higher MCMC iterations in practice (m)
pmmh_result <- pmmh(
  y = y,
  m = 1000,
  init_fn = init_fn,
  transition_fn = transition_fn,
  log_likelihood_fn = log_likelihood_fn,
  log_priors = log_priors,
  init_params = c(phi = 0.8, sigma_x = 1, sigma_y = 0.5),
  burn_in = 100,
  num_chains = 2,
  param_transform = list(
    phi = "identity",
    sigma_x = "log",
    sigma_y = "log"
  ),
  tune_control = default_tune_control(pilot_m = 500, pilot_burn_in = 100)
)
# Convergence warning is expected with such low MCMC iterations.

}
\references{
Andrieu et al. (2010). Particle Markov chain Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology),
72(3):269â€“342. doi: 10.1111/j.1467-9868.2009.00736.x
}
