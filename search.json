[{"path":"https://bjarkehautop.github.io/bayesSSM/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 bayesSSM authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/detailed-overview.html","id":"theoretical-background","dir":"Articles","previous_headings":"","what":"Theoretical Background","title":"Detailed Overview of bayesSSM Package","text":"bayesSSM package designed facilitate Bayesian inference state space models (SSMs). package defines state space model diagram (potential time dependency shown simplicity): wish perform Bayesian inference setting, target distribution posterior distribution latent states parameters given observations: \\begin{align*} p(x_{0:T}, \\theta \\mid y_{1:T}) &\\propto \\pi(\\theta) \\mu_\\theta(x_0) \\prod_{t=1}^T f_\\theta(x_t \\mid x_{t-1}) \\prod_{t=1}^T g_\\theta(y_t \\mid x_t) \\\\ &= \\pi(\\theta) p(x_{0:T} \\mid \\theta) p(y_{1:T} \\mid x_{0:T}, \\theta), \\end{align*} \\pi(\\theta) prior distribution parameters \\theta, p(x_{0:T} \\mid \\theta) distribution latent states, p(y_{1:T} \\mid x_{0:T}, \\theta) distribution observations given latent states parameters. Standard MCMC methods HMC can struggle explore high-dimensional correlated space efficiently. can also suffer degeneracy attempting jointly sample entire latent trajectories. Another approach marginalize latent states sample parameters \\theta, also often infeasible due integral intractable. Instead, use Particle Markov chain Monte Carlo (PMMH) approach, samples latent states parameters two steps: Sample latent states x_{0:T} given parameters \\theta observations y_{1:T} using particle filter. Sample parameters \\theta given latent states x_{0:T} observations y_{1:T} using Metropolis-Hastings step. key idea , since particle filter provides unbiased estimate likelihood, PMMH algorithm still targets correct posterior distribution stationary distribution. avoid weight degeneracy particle filter, recommended use resampling, focus particles higher weights (high probability).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/detailed-overview.html","id":"example-fitting-a-state-space-model","dir":"Articles","previous_headings":"","what":"Example: Fitting a State Space Model","title":"Detailed Overview of bayesSSM Package","text":"show fit stochastic SIR model using bayesSSM package ( see also stochastic-sir-model article details). First, generate synthetic data model.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/detailed-overview.html","id":"simulate-data","dir":"Articles","previous_headings":"Example: Fitting a State Space Model","what":"Simulate data","title":"Detailed Overview of bayesSSM Package","text":"simulate data SIR model following parameters: t=0 S(0) = 90, (0) = 10 R(0) = 0. Infection rate \\lambda=0.5 removal rate \\gamma=0.2. observe initial state t=0 complete, noisy version infectious individuals times t=1, \\ldots, 10 representing observations day. can simulate using fact two independent exponential distribution, event occurs rate sum rates. Now, generate data: plot :","code":"# --- Simulation settings and true parameters --- n_total <- 500 # Total population size init_infected <- 70 # Initially infectious individuals init_state <- c(n_total - init_infected, init_infected) # (s, i) at time 0 t_max <- 10 # Total number of days to simulate true_lambda <- 0.5 # True infection parameter true_gamma <- 0.2 # True removal parameter  # --- Functions for simulating the epidemic ---  epidemic_step <- function(state, lambda, gamma, n_total) {   t <- 0   t_end <- 1   s <- state[1]   i <- state[2]   while (t < t_end && i > 0) {     rate_infection <- (lambda / n_total) * s * i     rate_removal <- gamma * i     rate_total <- rate_infection + rate_removal     if (rate_total <= 0) break     dt <- rexp(1, rate_total)     if (t + dt > t_end) break     t <- t + dt     # Decide which event occurs:     if (runif(1) < rate_infection / rate_total) {       # Infection event       s <- s - 1       i <- i + 1     } else {       # Removal event       i <- i - 1     }   }   c(s, i) }  simulate_epidemic <- function(   n_total, init_infected, lambda, gamma, t_max ) {   states <- matrix(0, nrow = t_max, ncol = 2)   # initial state at t = 0   state <- c(n_total - init_infected, init_infected)   for (t in 1:t_max) {     state <- epidemic_step(state, lambda, gamma, n_total)     states[t, ] <- state   }   states } # Simulate an epidemic dataset true_states <- simulate_epidemic(   n_total, init_infected, true_lambda, true_gamma, t_max ) latent_i <- true_states[, 2]  observations <- rpois(length(latent_i), lambda = latent_i)  # Display simulated data: time, susceptible, latent infectious, observed counts print(data.frame(   time = 1:t_max, s = true_states[, 1], i = true_states[, 2], y = observations )) #>    time   s   i   y #> 1     1 391  88 103 #> 2     2 348 113 106 #> 3     3 307 132 114 #> 4     4 266 147 136 #> 5     5 221 164 155 #> 6     6 183 171 168 #> 7     7 155 159 154 #> 8     8 137 143 137 #> 9     9 118 139 147 #> 10   10 107 121 123 # Function to create a tidy dataset for ggplot prepare_data_for_plot <- function(states, observations, t_max) {   # Organize the data into a tidy format   data <- data.frame(     time = 1:t_max,     s = states[, 1],     i = states[, 2],     y = observations   )    # Convert to long format for ggplot   data_long <- data %>%     pivot_longer(       cols = c(\"s\", \"i\", \"y\"),       names_to = \"state\",       values_to = \"count\"     )    data_long }  # Function to plot the epidemic data plot_epidemic_data <- function(data_long, t_max) {   ggplot(data_long, aes(x = .data$time, y = .data$count, color = .data$state)) +     geom_line(linewidth = 1.2) +     scale_color_manual(values = c(\"s\" = \"blue\", \"i\" = \"red\", \"y\" = \"green\")) +     labs(       x = \"Time (Days)\",       y = \"Count\",       title = \"Susceptible, Infected, and Observed Counts\"     ) +     theme_minimal() +     theme(legend.title = element_blank()) +     scale_x_continuous(breaks = 1:t_max) +     theme(       axis.title = element_text(size = 12),       plot.title = element_text(size = 14, hjust = 0.5)     ) }  # Prepare data for plotting data_long <- prepare_data_for_plot(true_states, observations, t_max)  # Plot the results plot_epidemic_data(data_long, t_max)"},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/detailed-overview.html","id":"fitting-the-model","dir":"Articles","previous_headings":"Example: Fitting a State Space Model","what":"Fitting the model","title":"Detailed Overview of bayesSSM Package","text":"define initialization, transition, log-likelihood functions SIR model. Note, define variables used functions inside functions, can used parallel processing (see also tips tricks section ). initialization function init_fn must take argument num_particles return matrix particles, row corresponds particle’s initial state (susceptible infected individuals). example use deterministic initialization fixed number susceptible infected individuals. transition function transition_fn must take argument particles return matrix particles, row corresponds particle’s state next time step. log-likelihood function log_likelihood_fn must take arguments y particles, return vector log-likelihood values particle. time dependency can implemented giving t argument transition_fn log_likelihood_fn. example, time dependency, use t argument. priors \\lambda, \\gamma \\begin{align*}     \\lambda &\\sim \\operatorname{Normal^+}(0, 1^2), \\\\     \\gamma &\\sim \\operatorname{Normal^+}(0, 2^2). \\end{align*} Now can run PMMH algorithm estimate posterior distribution (modify tuning draw 1000 samples speed ).","code":"init_fn_epidemic <- function(num_particles) {   # Return a matrix with num_particles rows   # each row is the initial state (s, i)   n_total <- 500   init_infected <- 70   init_state <- c(S = n_total - init_infected, I = init_infected)   matrix(     rep(init_state, each = num_particles),     nrow = num_particles,     byrow = FALSE   ) }  transition_fn_epidemic <- function(particles, lambda, gamma, t) {   n_total <- 500    epidemic_step <- function(state, lambda, gamma, n_total) {     t <- 0     t_end <- 1     s <- state[1]     i <- state[2]     while (t < t_end && i > 0) {       rate_infection <- (lambda / n_total) * s * i       rate_removal <- gamma * i       rate_total <- rate_infection + rate_removal       if (rate_total <= 0) break       dt <- rexp(1, rate_total)       if (t + dt > t_end) break       t <- t + dt       # Decide which event occurs:       if (runif(1) < rate_infection / rate_total) {         # Infection event         s <- s - 1         i <- i + 1       } else {         # Removal event         i <- i - 1       }     }     c(s, i)   }    new_particles <- t(apply(particles, 1, function(state) {     s <- state[1]     i <- state[2]     if (i == 0) {       return(c(s, i))     }     epidemic_step(state, lambda, gamma, n_total)   }))   new_particles }  log_likelihood_fn_epidemic <- function(y, particles, t) {   # particles is expected to be a matrix with columns (s, i)   dpois(y, lambda = particles[, 2], log = TRUE) } log_prior_lambda <- function(lambda) {   extraDistr::dhnorm(lambda, sigma = 1, log = TRUE) }  log_prior_gamma <- function(gamma) {   extraDistr::dhnorm(gamma, sigma = 2, log = TRUE) }  log_priors <- list(   lambda = log_prior_lambda,   gamma = log_prior_gamma ) result <- pmmh(   pf_wrapper = bootstrap_filter, # use bootstrap particle filter   y = observations,   m = 1000,   init_fn = init_fn_epidemic,   transition_fn = transition_fn_epidemic,   log_likelihood_fn = log_likelihood_fn_epidemic,   log_priors = log_priors,   pilot_init_params = list(     c(lambda = 0.5, gamma = 0.5),     c(lambda = 1, gamma = 1)   ),   burn_in = 200,   num_chains = 2,   tune_control = default_tune_control(pilot_m = 100, pilot_burn_in = 10),   verbose = TRUE,   seed = 1405 ) #> Running chain 1... #> Running pilot chain for tuning... #> Pilot chain posterior mean: #>    lambda     gamma  #> 0.6953863 0.2245241 #> Pilot chain posterior covariance: #>               lambda         gamma #> lambda  1.156114e-03 -3.125054e-06 #> gamma  -3.125054e-06  8.447229e-09 #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> Running chain 2... #> Running pilot chain for tuning... #> Pilot chain posterior mean: #>    lambda     gamma  #> 0.4852500 0.2072084 #> Pilot chain posterior covariance: #>             lambda       gamma #> lambda 0.006865806 0.002023443 #> gamma  0.002023443 0.000596335 #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> PMMH Results Summary: #>  Parameter Mean   SD Median 2.5% 97.5% ESS  Rhat #>     lambda 0.57 0.08   0.57 0.42  0.71   2 1.262 #>      gamma 0.22 0.01   0.22 0.19  0.25  68 1.055 #> Warning in pmmh(pf_wrapper = bootstrap_filter, y = observations, m = 1000, : #> Some ESS values are below 400, indicating poor mixing. Consider running the #> chains for more iterations. #> Warning in pmmh(pf_wrapper = bootstrap_filter, y = observations, m = 1000, :  #> Some Rhat values are above 1.01, indicating that the chains have not converged.  #> Consider running the chains for more iterations and/or increase burn_in."},{"path":[]},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/detailed-overview.html","id":"using-several-cores","dir":"Articles","previous_headings":"Tips and Tricks","what":"Using Several Cores","title":"Detailed Overview of bayesSSM Package","text":"use several cores (number chains), can set num_cores argument pmmh function. code run PMMH algorithm using 2 cores, useful speeding sampling process, especially larger datasets complex models. Note, global variables must explicitly defined within functions exported worker processes. .e, n_total, init_infected, init_state variables must defined within init_fn_epidemic, transition_fn_epidemic, log_likelihood_fn_epidemic functions, shown .","code":"result <- pmmh(   pf_wrapper = bootstrap_filter, # use bootstrap particle filter   y = observations,   m = 1000,   init_fn = init_fn_epidemic,   transition_fn = transition_fn_epidemic,   log_likelihood_fn = log_likelihood_fn_epidemic,   log_priors = log_priors,   pilot_init_params = list(     c(lambda = 0.5, gamma = 0.5),     c(lambda = 1, gamma = 1)   ),   burn_in = 200,   num_chains = 2,   tune_control = default_tune_control(pilot_m = 100, pilot_burn_in = 10),   verbose = TRUE,   seed = 1405,   num_cores = 2 )"},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/detailed-overview.html","id":"sampling-on-an-unconstrained-space","dir":"Articles","previous_headings":"Tips and Tricks","what":"Sampling on an Unconstrained Space","title":"Detailed Overview of bayesSSM Package","text":"allow efficient sampling, can use param_transform argument propose new parameters unconstrained space, leading efficient sampling. Since \\lambda \\gamma take values (0, \\infty), can use log transformation.","code":"result <- pmmh(   pf_wrapper = bootstrap_filter, # use bootstrap particle filter   y = observations,   m = 1000,   init_fn = init_fn_epidemic,   transition_fn = transition_fn_epidemic,   log_likelihood_fn = log_likelihood_fn_epidemic,   log_priors = log_priors,   pilot_init_params = list(     c(lambda = 0.5, gamma = 0.5),     c(lambda = 1, gamma = 1)   ),   burn_in = 200,   num_chains = 2,   tune_control = default_tune_control(pilot_m = 100, pilot_burn_in = 10),   verbose = TRUE,   seed = 1405,   param_transform = list(lambda = \"log\", gamma = \"log\") )"},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/detailed-overview.html","id":"using-rcpp","dir":"Articles","previous_headings":"Tips and Tricks","what":"Using Rcpp","title":"Detailed Overview of bayesSSM Package","text":"transition log-likelihood functions computationally expensive, can speed sampling process using Rcpp implement functions C++. example transition function SIR model. C++ implementation much faster R implementation, speed PMMH sampling process significantly, especially larger datasets complex models. Now, can use C++ function pmmh function passing transition_fn argument.","code":"cppFunction(\" NumericMatrix transition_fn_epidemic_cpp(     NumericMatrix particles,     double lambda,     double gamma ) {    int n_particles = particles.nrow();   int n_total = 500;   double t_end = 1.0; // Time step for the transition   NumericMatrix new_particles(n_particles, 2);    for (int p = 0; p < n_particles; p++) {     int s = particles(p, 0);     int i = particles(p, 1);      if (i == 0) {       new_particles(p, 0) = s;       new_particles(p, 1) = i;       continue;     }      double t = 0.0;      while (t < t_end && i > 0) {       double rate_infection = (lambda / n_total) * s * i;       double rate_removal = gamma * i;       double rate_total = rate_infection + rate_removal;        if (rate_total <= 0.0) {         break;       }        double dt = R::rexp(1.0 / rate_total);       if (t + dt > t_end) {         break;       }        t += dt;        if (R::runif(0.0, 1.0) < (rate_infection / rate_total)) {         // Infection event         s -= 1;         i += 1;       } else {         // Removal event         i -= 1;       }     }      new_particles(p, 0) = s;     new_particles(p, 1) = i;   }    return new_particles; } \") result_cpp <- pmmh(   pf_wrapper = bootstrap_filter, # use bootstrap particle filter   y = observations,   m = 1000,   init_fn = init_fn_epidemic,   transition_fn = transition_fn_epidemic_cpp,   log_likelihood_fn = log_likelihood_fn_epidemic,   log_priors = log_priors,   pilot_init_params = list(     c(lambda = 0.5, gamma = 0.5),     c(lambda = 1, gamma = 1)   ),   burn_in = 200,   num_chains = 2,   tune_control = default_tune_control(pilot_m = 100, pilot_burn_in = 10),   verbose = TRUE,   seed = 1405,   param_transform = list(lambda = \"log\", gamma = \"log\") ) #> Running chain 1... #> Running pilot chain for tuning... #> Pilot chain posterior mean: #>    lambda     gamma  #> 0.5317352 0.1822626 #> Pilot chain posterior covariance (transformed space): #>             lambda       gamma #> lambda 0.015968002 0.004597379 #> gamma  0.004597379 0.001364845 #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> Running chain 2... #> Running pilot chain for tuning... #> Pilot chain posterior mean: #>    lambda     gamma  #> 0.4623588 0.1752997 #> Pilot chain posterior covariance (transformed space): #>               lambda         gamma #> lambda  0.0019022254 -2.446075e-04 #> gamma  -0.0002446075  4.555458e-05 #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> PMMH Results Summary: #>  Parameter Mean   SD Median 2.5% 97.5% ESS  Rhat #>     lambda 0.59 0.08   0.59 0.43  0.75  39 1.089 #>      gamma 0.21 0.02   0.22 0.16  0.25  15 1.082 #> Warning in pmmh(pf_wrapper = bootstrap_filter, y = observations, m = 1000, : #> Some ESS values are below 400, indicating poor mixing. Consider running the #> chains for more iterations. #> Warning in pmmh(pf_wrapper = bootstrap_filter, y = observations, m = 1000, :  #> Some Rhat values are above 1.01, indicating that the chains have not converged.  #> Consider running the chains for more iterations and/or increase burn_in."},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/detailed-overview.html","id":"observation-times","dir":"Articles","previous_headings":"Tips and Tricks","what":"Observation Times","title":"Detailed Overview of bayesSSM Package","text":"bayesSSM package allows specify observation times observations. useful observations equally spaced time. can specify observation times using obs_times argument pmmh function. observation times vector length observations, contain time points observations made. example use obs_times argument, assume observations made times 1, 2, 3 6.","code":"obs_times <- c(1, 2, 3, 6) new_observations <- observations[obs_times] result_obs_times <- pmmh(   pf_wrapper = bootstrap_filter, # use bootstrap particle filter   y = new_observations,   m = 1000,   init_fn = init_fn_epidemic,   transition_fn = transition_fn_epidemic,   log_likelihood_fn = log_likelihood_fn_epidemic,   log_priors = log_priors,   pilot_init_params = list(     c(lambda = 0.5, gamma = 0.5),     c(lambda = 1, gamma = 1)   ),   burn_in = 200,   num_chains = 2,   tune_control = default_tune_control(pilot_m = 100, pilot_burn_in = 10),   verbose = TRUE,   seed = 1405,   param_transform = list(lambda = \"log\", gamma = \"log\"),   obs_times = obs_times )"},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/detailed-overview.html","id":"specifying-prior-on-a-different-scale","dir":"Articles","previous_headings":"Tips and Tricks","what":"Specifying Prior on a Different Scale","title":"Detailed Overview of bayesSSM Package","text":"Sometimes, useful specify prior different scale parameter space. instance, common specify prior -dispersion parameter \\phi negative binomial model different scale,  \\frac{1}{\\sqrt{\\phi}} \\sim \\operatorname{Normal^+}(0, 1),  \\operatorname{Normal^+}(0, 1) truncated normal distribution positive real line. , need add log_jacobian transformation specified log-prior function. shown following function, implements prior \\phi scale \\frac{1}{\\sqrt{\\phi}}.","code":"log_prior_phi <- function(phi) {   if (phi <= 0) {     return(-Inf)   } # Ensure phi is positive    # Jacobian: |d(1/sqrt(phi))/dphi| = 1/(2 * phi^(3/2))   log_jacobian <- -log(2) - 1.5 * log(phi)   extraDistr::dhnorm(1 / sqrt(phi), sigma = 1, log = TRUE) + log_jacobian }"},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/detailed-overview.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Detailed Overview of bayesSSM Package","text":"article, provided detailed overview bayesSSM package use Bayesian inference state space models. shown define model, simulate data, fit model using PMMH algorithm. also provided tips tricks using package effectively, including use several cores, sample unconstrained space, use Rcpp faster sampling.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/stochastic-sir-model.html","id":"stochastic-sir-model","dir":"Articles","previous_headings":"","what":"Stochastic SIR Model","title":"Stochastic SIR Model","text":"consider stochastic SIR (Susceptible–Infectious–Recovered) model spread infectious disease closed population size pop_size, meaning births, deaths, migration. population divided three compartments: susceptible (S), infectious (), recovered (R). Initially, individuals susceptible except m infectious individuals. Note literature SIR models assumes deterministic model, compartments treated continuous described ordinary differential equations (ODEs). , instead consider fully stochastic model, compartments discrete transitions compartments governed probabilistic rules. Especially small populations, stochastic models can capture inherent randomness disease transmission recovery accurately deterministic models. infectious individual makes contact given member population according time-homogeneous Poisson process rate \\lambda/pop_size, \\lambda > 0. Thus, infectious individual makes contact individual overall rate \\lambda, independent population size. duration infectious period individual assumed follow exponential distribution:  \\sim \\text{Exp}(\\gamma),  \\gamma > 0 recovery rate. Let S(t) (t) denote number susceptible infectious individuals time t \\geq 0, respectively. assumptions , process  \\{(S(t), (t)) : t \\geq 0\\}  forms continuous-time Markov process, since infection recovery events exponentially distributed therefore memoryless. Since pop_size fixed, R(t) = pop_size - S(t) - (t), thus process can described pair (S(t), (t)).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/stochastic-sir-model.html","id":"transition-rates","dir":"Articles","previous_headings":"Stochastic SIR Model","what":"Transition Rates","title":"Stochastic SIR Model","text":"Infection Event: (s, , r) \\(s - 1, + 1, r) occurs rate \\frac{\\lambda}{pop_size} \\, s \\, , s > 0 > 0. Recovery Event: (s, , r) \\(s, - 1, r + 1) occurs rate \\gamma \\, , > 0.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/stochastic-sir-model.html","id":"partial-observations-and-noisy-measurements","dir":"Articles","previous_headings":"Stochastic SIR Model","what":"Partial Observations and Noisy Measurements","title":"Stochastic SIR Model","text":"Suppose observe initial state number infectious individuals (t) discrete time points t = 0, 1, \\ldots, T. true number infectious individuals unobserved (latent), instead observe noisy measurement may either overestimate underestimate true count. model observed counts Y_t article using Poisson observation model:  Y_t \\mid (t) \\sim \\operatorname{Pois}((t)).  account -dispersion, one instead use negative binomial observation model. now State Space Model (SSM) setup, latent state \\bigl(S(t), (t)\\bigr), observations Y_t. Note due memoryless property exponential distribution, latent state (S(t), (t)) fully determined initial state sequence infection recovery events occurred time t, thus Markov process.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/stochastic-sir-model.html","id":"why-standard-mcmc-methods-are-infeasible","dir":"Articles","previous_headings":"","what":"Why Standard MCMC Methods Are Infeasible","title":"Stochastic SIR Model","text":"Due high dimensionality complex dependencies latent state space, standard MCMC methods Hamiltonian Monte Carlo (HMC) struggle explore posterior distribution efficiently. Moreover, methods require evaluating transition density p(x_{t+1} \\mid x_t, \\theta), requires solving Kolmogorov forward equation. tractable small populations, making direct likelihood-based MCMC infeasible practice.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/stochastic-sir-model.html","id":"simulate-data","dir":"Articles","previous_headings":"","what":"Simulate data","title":"Stochastic SIR Model","text":"simulate data SIR model following parameters: t=0 S(0) = 90, (0) = 10 R(0) = 0. Infection rate \\lambda=1.5 removal rate \\gamma=0.5. observe initial state t=0 complete, noisy version infectious individuals times t=1, \\ldots, 10 representing observations day. can simulate using fact two independent exponential distribution, event occurs rate sum rates. Now, generate data: plot :","code":"# --- Simulation settings and true parameters --- n_total <- 500 # Total population size init_infected <- 70 # Initially infectious individuals init_state <- c(n_total - init_infected, init_infected) # (s, i) at time 0 t_max <- 10 # Total number of days to simulate true_lambda <- 0.5 # True infection parameter true_gamma <- 0.2 # True removal parameter  # --- Functions for simulating the epidemic ---  epidemic_step <- function(state, lambda, gamma, n_total) {   t <- 0   t_end <- 1   s <- state[1]   i <- state[2]   while (t < t_end && i > 0) {     rate_infection <- (lambda / n_total) * s * i     rate_removal <- gamma * i     rate_total <- rate_infection + rate_removal     if (rate_total <= 0) break     dt <- rexp(1, rate_total)     if (t + dt > t_end) break     t <- t + dt     # Decide which event occurs:     if (runif(1) < rate_infection / rate_total) {       # Infection event       s <- s - 1       i <- i + 1     } else {       # Removal event       i <- i - 1     }   }   c(s, i) }  simulate_epidemic <- function(   n_total, init_infected, lambda, gamma, t_max ) {   states <- matrix(0, nrow = t_max, ncol = 2)   # initial state at t = 0   state <- c(n_total - init_infected, init_infected)   for (t in 1:t_max) {     state <- epidemic_step(state, lambda, gamma, n_total)     states[t, ] <- state   }   states } # Simulate an epidemic dataset true_states <- simulate_epidemic(   n_total, init_infected, true_lambda, true_gamma, t_max ) latent_i <- true_states[, 2]  observations <- rpois(length(latent_i), lambda = latent_i)  # Display simulated data: time, susceptible, latent infectious, observed counts print(data.frame(   time = 1:t_max, s = true_states[, 1], i = true_states[, 2], y = observations )) #>    time   s   i   y #> 1     1 391  88 103 #> 2     2 348 113 106 #> 3     3 307 132 114 #> 4     4 266 147 136 #> 5     5 221 164 155 #> 6     6 183 171 168 #> 7     7 155 159 154 #> 8     8 137 143 137 #> 9     9 118 139 147 #> 10   10 107 121 123 # Function to create a tidy dataset for ggplot prepare_data_for_plot <- function(states, observations, t_max) {   # Organize the data into a tidy format   data <- data.frame(     time = 1:t_max,     s = states[, 1],     i = states[, 2],     y = observations   )    # Convert to long format for ggplot   data_long <- data %>%     gather(key = \"state\", value = \"count\", -time)    data_long }  # Function to plot the epidemic data plot_epidemic_data <- function(data_long, t_max) {   ggplot(data_long, aes(x = .data$time, y = .data$count, color = .data$state)) +     geom_line(linewidth = 1.2) +     scale_color_manual(values = c(\"s\" = \"blue\", \"i\" = \"red\", \"y\" = \"green\")) +     labs(       x = \"Time (Days)\", y = \"Count\",       title = \"Susceptible, Infected, and Observed Counts\"     ) +     theme_minimal() +     theme(legend.title = element_blank()) +     scale_x_continuous(breaks = 1:t_max) +     theme(       axis.title = element_text(size = 12),       plot.title = element_text(size = 14, hjust = 0.5)     ) }  # Prepare data for plotting data_long <- prepare_data_for_plot(true_states, observations, t_max)  # Plot the results plot_epidemic_data(data_long, t_max)"},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/stochastic-sir-model.html","id":"bayesian-inference-with-pmmh","dir":"Articles","previous_headings":"","what":"Bayesian Inference with PMMH","title":"Stochastic SIR Model","text":"interested performing Bayesian inference setup, define priors follows: \\begin{align*}     \\lambda &\\sim \\operatorname{Normal^+}(0, 1^2), \\\\     \\gamma &\\sim \\operatorname{Normal^+}(0, 2^2), \\end{align*} \\operatorname{Normal^+} denotes truncated 0 normal distribution. now define initial state, transition likelihood functions SIR model. Now can run PMMH algorithm estimate posterior distribution. vignette use small number iterations (1000) 2 chains (also modify tuning use 100 iterations burn-10). practice, much higher. get convergence warnings expected, posterior still centered around true value. can access chains plot densities: \\lambda:  \\gamma:","code":"# Define the log-prior for the parameters log_prior_lambda <- function(lambda) {   extraDistr::dhnorm(lambda, sigma = 1, log = TRUE) }  log_prior_gamma <- function(gamma) {   extraDistr::dhnorm(gamma, sigma = 2, log = TRUE) }  log_priors <- list(   lambda = log_prior_lambda,   gamma = log_prior_gamma ) init_fn_epidemic <- function(num_particles) {   # Return a matrix with particles rows; each row is the initial state (s, i)   matrix(     rep(init_state, each = num_particles),     nrow = num_particles,     byrow = FALSE   ) }  transition_fn_epidemic <- function(particles, lambda, gamma, t) {   new_particles <- t(apply(particles, 1, function(state) {     s <- state[1]     i <- state[2]     if (i == 0) {       return(c(s, i))     }     epidemic_step(state, lambda, gamma, n_total)   }))   new_particles }  log_likelihood_fn_epidemic <- function(y, particles) {   # particles is expected to be a matrix with columns (s, i)   dpois(y, lambda = particles[, 2], log = TRUE) } result <- bayesSSM::pmmh(   pf_wrapper = bootstrap_filter, # use bootstrap particle filter   y = observations,   m = 1000,   init_fn = init_fn_epidemic,   transition_fn = transition_fn_epidemic,   log_likelihood_fn = log_likelihood_fn_epidemic,   log_priors = log_priors,   pilot_init_params = list(     c(lambda = 0.5, gamma = 0.5),     c(lambda = 1, gamma = 1)   ),   burn_in = 200,   num_chains = 2,   param_transform = list(lambda = \"log\", gamma = \"log\"),   tune_control = default_tune_control(pilot_m = 100, pilot_burn_in = 10),   verbose = TRUE,   seed = 1405, ) #> Running chain 1... #> Running pilot chain for tuning... #> Pilot chain posterior mean: #>    lambda     gamma  #> 0.5317352 0.1822626 #> Pilot chain posterior covariance (transformed space): #>             lambda       gamma #> lambda 0.015968002 0.004597379 #> gamma  0.004597379 0.001364845 #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> Running chain 2... #> Running pilot chain for tuning... #> Pilot chain posterior mean: #>    lambda     gamma  #> 0.4623588 0.1752997 #> Pilot chain posterior covariance (transformed space): #>               lambda         gamma #> lambda  0.0019022254 -2.446075e-04 #> gamma  -0.0002446075  4.555458e-05 #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> PMMH Results Summary: #>  Parameter Mean   SD Median 2.5% 97.5% ESS  Rhat #>     lambda 0.59 0.08   0.59 0.43  0.75  39 1.089 #>      gamma 0.21 0.02   0.22 0.16  0.25  15 1.082 #> Warning in bayesSSM::pmmh(pf_wrapper = bootstrap_filter, y = observations, : #> Some ESS values are below 400, indicating poor mixing. Consider running the #> chains for more iterations. #> Warning in bayesSSM::pmmh(pf_wrapper = bootstrap_filter, y = observations, :  #> Some Rhat values are above 1.01, indicating that the chains have not converged.  #> Consider running the chains for more iterations and/or increase burn_in. chains <- result$theta_chain ggplot(chains, aes(x = lambda, fill = factor(chain))) +   geom_density(alpha = 0.5) +   labs(     title = \"Density plot of lambda chains\",     x = \"Value\",     y = \"Density\",     fill = \"Chain\"   ) +   theme_minimal() ggplot(chains, aes(x = gamma, fill = factor(chain))) +   geom_density(alpha = 0.5) +   labs(     title = \"Density plot of gamma chains\",     x = \"Value\",     y = \"Density\",     fill = \"Chain\"   ) +   theme_minimal()"},{"path":"https://bjarkehautop.github.io/bayesSSM/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Bjarke Hautop. Author, maintainer, copyright holder.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hautop B (2025). bayesSSM: Bayesian Methods State Space Models. R package version 0.7.0.9002, https://github.com/BjarkeHautop/bayesSSM.","code":"@Manual{,   title = {bayesSSM: Bayesian Methods for State Space Models},   author = {Bjarke Hautop},   year = {2025},   note = {R package version 0.7.0.9002},   url = {https://github.com/BjarkeHautop/bayesSSM}, }"},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"bayesssm-","dir":"","previous_headings":"","what":"Bayesian Methods for State Space Models","title":"Bayesian Methods for State Space Models","text":"bayesSSM R package offering set tools performing Bayesian inference state-space models (SSMs). implements Particle Marginal Metropolis-Hastings (PMMH) main function pmmh Bayesian inference SSMs.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"why-bayesssm","dir":"","previous_headings":"","what":"Why bayesSSM?","title":"Bayesian Methods for State Space Models","text":"several alternative packages available performing Particle MCMC, bayesSSM designed simple easy use. developed alongside Master’s thesis Particle MCMC, since implementing everything scratch anyway.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"why-pmcmc","dir":"","previous_headings":"","what":"Why PMCMC?","title":"Bayesian Methods for State Space Models","text":"state-space model, joint posterior density parameters \\theta latent states x_{1:T} given observations y_{1:T}  Since dimension latent states increases number observations standard MCMC methods HMC can struggle explore high-dimensional correlated space efficiently. can also suffer degeneracy attempting jointly sample entire latent trajectories. Particle Markov Chain Monte Carlo (PMCMC) methods, Particle Marginal Metropolis-Hastings (PMMH) implemented package, designed handle situations. using particle filters unbiasedly estimate marginal likelihood, enable efficient sampling correct joint posterior avoiding need explicitly explore full latent state space.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"state-space-models","dir":"","previous_headings":"","what":"State-space Models","title":"Bayesian Methods for State Space Models","text":"state-space model (SSM) structure given following diagram, omitted potential time dependency transition observation densities simplicity.  core function, pmmh, implements Particle Marginal Metropolis-Hastings, algorithm first generates set N particles approximate intractable marginal likelihood uses approximation acceptance probability. implementation automatically tunes number particles proposal distribution parameters.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Bayesian Methods for State Space Models","text":"can install latest stable version bayesSSM CRAN : development version GitHub :","code":"install.packages(\"bayesSSM\") # install.packages(\"pak\") pak::pak(\"BjarkeHautop/bayesSSM\")"},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Bayesian Methods for State Space Models","text":"illustrate use pmmh function, simulate simple state-space model (SSM) perform Bayesian inference . Note: example uses pmmh, model simple enough standard MCMC methods also applied. complicated example standard MCMC methods struggle see article Stochastic SIR Model . simulate state-space model following structure:  Let’s first simulate 20 data points model \\phi = 0.8, \\sigma_x = 1, \\sigma_y = 0.5. define priors model follows:  can use pmmh perform Bayesian inference model. use pmmh need define functions SSM priors. functions init_fn, transition_fn functions simulate latent states. init_fn must contain argument num_particles initializing particles, transition_fn must contain argument particles, vector particles, can contain arguments model-specific parameters. function log_likelihood_fn function calculates log-likelihood observed data given latent state variables. must contain arguments y data particles. Time dependency can implemented giving t argument transition_fn log_likelihood_fn. priors parameters must defined log-prior functions. Every parameter init_fn, transition_fn, log_likelihood_fn must corresponding log-prior function. Now can run PMMH algorithm using pmmh function. README use lower number samples smaller burn-period, also modify pilot chains use 200 samples. done make example run faster. get convergence warnings expected due small number iterations.","code":"set.seed(1405) t_val <- 20 phi <- 0.8 sigma_x <- 1 sigma_y <- 0.5  init_state <- rnorm(1, mean = 0, sd = 1) x <- numeric(t_val) y <- numeric(t_val) x[1] <- phi * init_state + sin(init_state) +   rnorm(1, mean = 0, sd = sigma_x) y[1] <- x[1] + rnorm(1, mean = 0, sd = sigma_y) for (t in 2:t_val) {   x[t] <- phi * x[t - 1] + sin(x[t - 1]) + rnorm(1, mean = 0, sd = sigma_x)   y[t] <- x[t] + rnorm(1, mean = 0, sd = sigma_y) } x <- c(init_state, x) init_fn <- function(num_particles) {   rnorm(num_particles, mean = 0, sd = 1) } transition_fn <- function(particles, phi, sigma_x) {   phi * particles + sin(particles) +     rnorm(length(particles), mean = 0, sd = sigma_x) } log_likelihood_fn <- function(y, particles, sigma_y) {   dnorm(y, mean = particles, sd = sigma_y, log = TRUE) } log_prior_phi <- function(phi) {   dunif(phi, min = 0, max = 1, log = TRUE) } log_prior_sigma_x <- function(sigma) {   dexp(sigma, rate = 1, log = TRUE) } log_prior_sigma_y <- function(sigma) {   dexp(sigma, rate = 1, log = TRUE) }  log_priors <- list(   phi = log_prior_phi,   sigma_x = log_prior_sigma_x,   sigma_y = log_prior_sigma_y ) library(bayesSSM)  result <- pmmh(   pf_wrapper = bootstrap_filter, # use bootstrap particle filter   y = y,   m = 500, # number of MCMC samples   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   log_priors = log_priors,   pilot_init_params = list(     c(phi = 0.4, sigma_x = 0.4, sigma_y = 0.4),     c(phi = 0.8, sigma_x = 0.8, sigma_y = 0.8)   ),   burn_in = 50,   num_chains = 2,   seed = 1405,   tune_control = default_tune_control(pilot_m = 200, pilot_burn_in = 10) ) #> Running chain 1... #> Running pilot chain for tuning... #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> Running chain 2... #> Running pilot chain for tuning... #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> PMMH Results Summary: #>  Parameter Mean   SD Median 2.5% 97.5% ESS  Rhat #>        phi 0.76 0.12   0.75 0.55  0.97   8 1.478 #>    sigma_x 0.78 0.56   0.74 0.01  1.85  15 1.093 #>    sigma_y 0.89 0.36   0.94 0.22  1.45  36 1.051 #> Warning in pmmh(pf_wrapper = bootstrap_filter, y = y, m = 500, init_fn = #> init_fn, : Some ESS values are below 400, indicating poor mixing. Consider #> running the chains for more iterations. #> Warning in pmmh(pf_wrapper = bootstrap_filter, y = y, m = 500, init_fn = init_fn, :  #> Some Rhat values are above 1.01, indicating that the chains have not converged.  #> Consider running the chains for more iterations and/or increase burn_in."},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/auxiliary_filter.html","id":null,"dir":"Reference","previous_headings":"","what":"Auxiliary Particle Filter (APF) — auxiliary_filter","title":"Auxiliary Particle Filter (APF) — auxiliary_filter","text":"Auxiliary Particle Filter differs bootstrap filter incorporating look-ahead step: particles reweighted using approximation likelihood next observation prior resampling. adjustment can help reduce particle degeneracy , improve filtering efficiency compared bootstrap approach.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/auxiliary_filter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Auxiliary Particle Filter (APF) — auxiliary_filter","text":"","code":"auxiliary_filter(   y,   num_particles,   init_fn,   transition_fn,   log_likelihood_fn,   aux_log_likelihood_fn,   obs_times = NULL,   resample_algorithm = c(\"SISAR\", \"SISR\", \"SIS\"),   resample_fn = c(\"stratified\", \"systematic\", \"multinomial\"),   threshold = NULL,   return_particles = TRUE,   ... )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/auxiliary_filter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Auxiliary Particle Filter (APF) — auxiliary_filter","text":"y numeric vector matrix observations. row represents observation time step. observations equally spaced, use obs_times argument. num_particles positive integer specifying number particles. init_fn function initialize particles. take `num_particles` return matrix vector initial states. Additional model parameters can passed via .... transition_fn function propagating particles. take `particles` optionally `t`. Additional model parameters via .... log_likelihood_fn function returns log-likelihood particle given current observation, particles, optionally `t`. Additional parameters via .... aux_log_likelihood_fn function computes log-likelihood next observation given current particles. accept arguments `y`, `particles`, optionally `t`, additional model-specific parameters via .... returns numeric vector log-likelihoods. obs_times numeric vector specifying observation time points. Must match number rows y, defaults 1:nrow(y). resample_algorithm character string specifying filtering resample algorithm: \"SIS\" resampling, \"SISR\" resampling every time step, \"SISAR\" adaptive resampling ESS drops threshold. Using \"SISR\" \"SISAR\" avoid weight degeneracy recommended. Default \"SISAR\". resample_fn string indicating resampling method: \"stratified\", \"systematic\", \"multinomial\". Default \"stratified\". threshold numeric value specifying ESS threshold \"SISAR\". Defaults num_particles / 2. return_particles Logical; TRUE, returns full particle weight histories. ... Additional arguments passed init_fn, transition_fn, log_likelihood_fn.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/auxiliary_filter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Auxiliary Particle Filter (APF) — auxiliary_filter","text":"list components: state_est Estimated states time (weighted mean particles). ess Effective sample size time step. loglike Total log-likelihood. loglike_history Log-likelihood time step. algorithm filtering algorithm used. particles_history Matrix particle states time   (return_particles = TRUE). weights_history Matrix particle weights time   (return_particles = TRUE).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/auxiliary_filter.html","id":"the-auxiliary-particle-filter-apf-","dir":"Reference","previous_headings":"","what":"The Auxiliary Particle Filter (APF)","title":"Auxiliary Particle Filter (APF) — auxiliary_filter","text":"Auxiliary Particle Filter (APF) introduced Pitt Shephard (1999) improve upon standard bootstrap filter incorporating look ahead step. resampling time \\(t\\), particles weighted auxiliary weight proportional estimate likelihood next observation, guiding resampling favor particles likely contribute future predictions. Specifically, \\(w_{t-1}^\\) normalized weights \\(x_{t-1}^\\) particles time \\(t-1\\), auxiliary weights computed $$   \\tilde{w}_t^\\propto w_{t-1}^\\, p(y_t | \\mu_t^), $$ \\(\\mu_t^\\) predictive summary (e.g., expected next state) particle \\(x_{t-1}^\\). Resampling performed using \\(\\tilde{w}_t^\\) instead \\(w_{t-1}^\\). can reduce variance importance weights time \\(t\\) help mitigate particle degeneracy, especially auxiliary weights chosen well. Default resampling method stratified resampling, lower variance multinomial resampling (Douc et al., 2005).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/auxiliary_filter.html","id":"model-specification","dir":"Reference","previous_headings":"","what":"Model Specification","title":"Auxiliary Particle Filter (APF) — auxiliary_filter","text":"Particle filter implementations package assume discrete-time state-space model defined : sequence latent states \\(x_0, x_1, \\ldots, x_T\\) evolving   according Markov process. Observations \\(y_1, \\ldots, y_T\\) conditionally   independent given corresponding latent states. model specified : $$x_0 \\sim \\mu_\\theta$$ $$x_t \\sim f_\\theta(x_t \\mid x_{t-1}), \\quad t = 1, \\ldots, T$$ $$y_t \\sim g_\\theta(y_t \\mid x_t), \\quad t = 1, \\ldots, T$$ \\(\\theta\\) denotes model parameters passed via .... user provides following functions: init_fn: draws initial distribution   \\(\\mu_\\theta\\). transition_fn: generates evaluates transition   density \\(f_\\theta\\). weight_fn: evaluates observation likelihood   \\(g_\\theta\\).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/auxiliary_filter.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Auxiliary Particle Filter (APF) — auxiliary_filter","text":"Pitt, M. K., & Shephard, N. (1999). Filtering via simulation: Auxiliary particle filters. Journal American Statistical Association, 94(446), 590–599. doi:10.1080/01621459.1999.10474153 Douc, R., Cappé, O., & Moulines, E. (2005). Comparison Resampling Schemes Particle Filtering. Accessible : https://arxiv.org/abs/cs/0507025","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/auxiliary_filter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Auxiliary Particle Filter (APF) — auxiliary_filter","text":"","code":"init_fn <- function(num_particles) rnorm(num_particles, 0, 1) transition_fn <- function(particles) particles + rnorm(length(particles)) log_likelihood_fn <- function(y, particles) {   dnorm(y, mean = particles, sd = 1, log = TRUE) } aux_log_likelihood_fn <- function(y, particles) {   # Predict next state (mean stays same) and compute log p(y | x)   mean_forecast <- particles # since E[x'] = x in this model   dnorm(y, mean = mean_forecast, sd = 1, log = TRUE) }  y <- cumsum(rnorm(50)) # dummy data num_particles <- 100  result <- auxiliary_filter(   y = y,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   aux_log_likelihood_fn = aux_log_likelihood_fn ) plot(result$state_est,   type = \"l\", col = \"blue\", main = \"APF: State Estimates\",   ylim = range(c(result$state_est, y)) ) points(y, col = \"red\", pch = 20)   # ---- With parameters ---- init_fn <- function(num_particles) rnorm(num_particles, 0, 1) transition_fn <- function(particles, mu) {   particles + rnorm(length(particles), mean = mu) } log_likelihood_fn <- function(y, particles, sigma) {   dnorm(y, mean = particles, sd = sigma, log = TRUE) } aux_log_likelihood_fn <- function(y, particles, mu, sigma) {   # Forecast mean of x' given x, then evaluate p(y | forecast)   forecast <- particles + mu   dnorm(y, mean = forecast, sd = sigma, log = TRUE) }  y <- cumsum(rnorm(50)) # dummy data num_particles <- 100  result <- auxiliary_filter(   y = y,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   aux_log_likelihood_fn = aux_log_likelihood_fn,   mu = 1,   sigma = 1 ) plot(result$state_est,   type = \"l\", col = \"blue\", main = \"APF with Parameters\",   ylim = range(c(result$state_est, y)) ) points(y, col = \"red\", pch = 20)   # ---- With observation gaps ---- simulate_ssm <- function(num_steps, mu, sigma) {   x <- numeric(num_steps)   y <- numeric(num_steps)   x[1] <- rnorm(1, mean = 0, sd = sigma)   y[1] <- rnorm(1, mean = x[1], sd = sigma)   for (t in 2:num_steps) {     x[t] <- mu * x[t - 1] + sin(x[t - 1]) + rnorm(1, mean = 0, sd = sigma)     y[t] <- x[t] + rnorm(1, mean = 0, sd = sigma)   }   y }  data <- simulate_ssm(10, mu = 1, sigma = 1) obs_times <- c(1, 2, 3, 5, 6, 7, 8, 9, 10) # Missing at t = 4 data_obs <- data[obs_times]  init_fn <- function(num_particles) rnorm(num_particles, 0, 1) transition_fn <- function(particles, mu) {   particles + rnorm(length(particles), mean = mu) } log_likelihood_fn <- function(y, particles, sigma) {   dnorm(y, mean = particles, sd = sigma, log = TRUE) } aux_log_likelihood_fn <- function(y, particles, mu, sigma) {   forecast <- particles + mu   dnorm(y, mean = forecast, sd = sigma, log = TRUE) }  num_particles <- 100 result <- auxiliary_filter(   y = data_obs,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   aux_log_likelihood_fn = aux_log_likelihood_fn,   obs_times = obs_times,   mu = 1,   sigma = 1 ) plot(result$state_est,   type = \"l\", col = \"blue\", main = \"APF with Observation Gaps\",   ylim = range(c(result$state_est, data)) ) points(data_obs, col = \"red\", pch = 20)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bayesSSM-package.html","id":null,"dir":"Reference","previous_headings":"","what":"bayesSSM: Bayesian Inference for State-Space Models — bayesSSM-package","title":"bayesSSM: Bayesian Inference for State-Space Models — bayesSSM-package","text":"bayesSSM package provides implementations particle filtering, Particle MCMC, related methods Bayesian inference state-space models. includes tools simulation, posterior inference, diagnostics.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bayesSSM-package.html","id":"model-specification","dir":"Reference","previous_headings":"","what":"Model Specification","title":"bayesSSM: Bayesian Inference for State-Space Models — bayesSSM-package","text":"Particle filter implementations package assume discrete-time state-space model defined : sequence latent states \\(x_0, x_1, \\ldots, x_T\\) evolving   according Markov process. Observations \\(y_1, \\ldots, y_T\\) conditionally   independent given corresponding latent states. model specified : $$x_0 \\sim \\mu_\\theta$$ $$x_t \\sim f_\\theta(x_t \\mid x_{t-1}), \\quad t = 1, \\ldots, T$$ $$y_t \\sim g_\\theta(y_t \\mid x_t), \\quad t = 1, \\ldots, T$$ \\(\\theta\\) denotes model parameters passed via .... user provides following functions: init_fn: draws initial distribution   \\(\\mu_\\theta\\). transition_fn: generates evaluates transition   density \\(f_\\theta\\). weight_fn: evaluates observation likelihood   \\(g_\\theta\\).","code":""},{"path":[]},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bayesSSM-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"bayesSSM: Bayesian Inference for State-Space Models — bayesSSM-package","text":"Maintainer: Bjarke Hautop bjarke.hautop@gmail.com [copyright holder]","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bootstrap_filter.html","id":null,"dir":"Reference","previous_headings":"","what":"Bootstrap Particle Filter (BPF) — bootstrap_filter","title":"Bootstrap Particle Filter (BPF) — bootstrap_filter","text":"Implements bootstrap particle filter sequential Bayesian inference state space models using sequential Monte Carlo methods.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bootstrap_filter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bootstrap Particle Filter (BPF) — bootstrap_filter","text":"","code":"bootstrap_filter(   y,   num_particles,   init_fn,   transition_fn,   log_likelihood_fn,   obs_times = NULL,   resample_algorithm = c(\"SISAR\", \"SISR\", \"SIS\"),   resample_fn = c(\"stratified\", \"systematic\", \"multinomial\"),   threshold = NULL,   return_particles = TRUE,   ... )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bootstrap_filter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bootstrap Particle Filter (BPF) — bootstrap_filter","text":"y numeric vector matrix observations. row represents observation time step. observations equally spaced, use obs_times argument. num_particles positive integer specifying number particles. init_fn function initialize particles. take `num_particles` return matrix vector initial states. Additional model parameters can passed via .... transition_fn function propagating particles. take `particles` optionally `t`. Additional model parameters via .... log_likelihood_fn function returns log-likelihood particle given current observation, particles, optionally `t`. Additional parameters via .... obs_times numeric vector specifying observation time points. Must match number rows y, defaults 1:nrow(y). resample_algorithm character string specifying filtering resample algorithm: \"SIS\" resampling, \"SISR\" resampling every time step, \"SISAR\" adaptive resampling ESS drops threshold. Using \"SISR\" \"SISAR\" avoid weight degeneracy recommended. Default \"SISAR\". resample_fn string indicating resampling method: \"stratified\", \"systematic\", \"multinomial\". Default \"stratified\". threshold numeric value specifying ESS threshold \"SISAR\". Defaults num_particles / 2. return_particles Logical; TRUE, returns full particle weight histories. ... Additional arguments passed init_fn, transition_fn, log_likelihood_fn.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bootstrap_filter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bootstrap Particle Filter (BPF) — bootstrap_filter","text":"list components: state_est Estimated states time (weighted mean particles). ess Effective sample size time step. loglike Total log-likelihood. loglike_history Log-likelihood time step. algorithm filtering algorithm used. particles_history Matrix particle states time   (return_particles = TRUE). weights_history Matrix particle weights time   (return_particles = TRUE).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bootstrap_filter.html","id":"the-effective-sample-size-ess-is-defined-as","dir":"Reference","previous_headings":"","what":"The Effective Sample Size (ESS) is defined as","title":"Bootstrap Particle Filter (BPF) — bootstrap_filter","text":"$$ESS = \\left(\\sum_{=1}^{n} w_i^2\\right)^{-1},$$ \\(w_i\\) normalized weights particles. Default resampling method stratified resampling, lower variance multinomial resampling (Douc et al., 2005).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bootstrap_filter.html","id":"model-specification","dir":"Reference","previous_headings":"","what":"Model Specification","title":"Bootstrap Particle Filter (BPF) — bootstrap_filter","text":"Particle filter implementations package assume discrete-time state-space model defined : sequence latent states \\(x_0, x_1, \\ldots, x_T\\) evolving   according Markov process. Observations \\(y_1, \\ldots, y_T\\) conditionally   independent given corresponding latent states. model specified : $$x_0 \\sim \\mu_\\theta$$ $$x_t \\sim f_\\theta(x_t \\mid x_{t-1}), \\quad t = 1, \\ldots, T$$ $$y_t \\sim g_\\theta(y_t \\mid x_t), \\quad t = 1, \\ldots, T$$ \\(\\theta\\) denotes model parameters passed via .... user provides following functions: init_fn: draws initial distribution   \\(\\mu_\\theta\\). transition_fn: generates evaluates transition   density \\(f_\\theta\\). weight_fn: evaluates observation likelihood   \\(g_\\theta\\).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bootstrap_filter.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bootstrap Particle Filter (BPF) — bootstrap_filter","text":"Gordon, N. J., Salmond, D. J., & Smith, . F. M. (1993). Novel approach nonlinear/non-Gaussian Bayesian state estimation. IEE Proceedings F (Radar Signal Processing), 140(2), 107–113. doi:10.1049/ip-f-2.1993.0015 Douc, R., Cappé, O., & Moulines, E. (2005). Comparison Resampling Schemes Particle Filtering. Accessible : https://arxiv.org/abs/cs/0507025","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/bootstrap_filter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bootstrap Particle Filter (BPF) — bootstrap_filter","text":"","code":"init_fn <- function(num_particles) rnorm(num_particles, 0, 1) transition_fn <- function(particles) particles + rnorm(length(particles)) log_likelihood_fn <- function(y, particles) {   dnorm(y, mean = particles, sd = 1, log = TRUE) }  y <- cumsum(rnorm(50)) # dummy data num_particles <- 100  # Run the particle filter using default settings. result <- bootstrap_filter(   y = y,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn ) plot(result$state_est,   type = \"l\", col = \"blue\", main = \"State Estimates\",   ylim = range(c(result$state_est, y)) ) points(y, col = \"red\", pch = 20)   # With parameters init_fn <- function(num_particles) rnorm(num_particles, 0, 1) transition_fn <- function(particles, mu) {   particles + rnorm(length(particles), mean = mu) } log_likelihood_fn <- function(y, particles, sigma) {   dnorm(y, mean = particles, sd = sigma, log = TRUE) }  y <- cumsum(rnorm(50)) # dummy data num_particles <- 100  # Run the bootstrap particle filter using default settings. result <- bootstrap_filter(   y = y,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   mu = 1,   sigma = 1 ) plot(result$state_est,   type = \"l\", col = \"blue\", main = \"State Estimates\",   ylim = range(c(result$state_est, y)) ) points(y, col = \"red\", pch = 20)   # With observations gaps init_fn <- function(num_particles) rnorm(num_particles, 0, 1) transition_fn <- function(particles, mu) {   particles + rnorm(length(particles), mean = mu) } log_likelihood_fn <- function(y, particles, sigma) {   dnorm(y, mean = particles, sd = sigma, log = TRUE) }  # Generate data using DGP simulate_ssm <- function(num_steps, mu, sigma) {   x <- numeric(num_steps)   y <- numeric(num_steps)   x[1] <- rnorm(1, mean = 0, sd = sigma)   y[1] <- rnorm(1, mean = x[1], sd = sigma)   for (t in 2:num_steps) {     x[t] <- mu * x[t - 1] + sin(x[t - 1]) + rnorm(1, mean = 0, sd = sigma)     y[t] <- x[t] + rnorm(1, mean = 0, sd = sigma)   }   y }  data <- simulate_ssm(10, mu = 1, sigma = 1) # Suppose we have data for t=1,2,3,5,6,7,8,9,10 (i.e., missing at t=4)  obs_times <- c(1, 2, 3, 5, 6, 7, 8, 9, 10) data_obs <- data[obs_times]  num_particles <- 100 # Specify observation times in the bootstrap particle filter using obs_times result <- bootstrap_filter(   y = data_obs,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   obs_times = obs_times,   mu = 1,   sigma = 1, ) plot(result$state_est,   type = \"l\", col = \"blue\", main = \"State Estimates\",   ylim = range(c(result$state_est, data)) ) points(data_obs, col = \"red\", pch = 20)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/default_tune_control.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Tuning Control Parameters — default_tune_control","title":"Create Tuning Control Parameters — default_tune_control","text":"function creates list tuning parameters used pmmh function. tuning choices inspired Pitt et al. [2012] Dahlin Schön [2019].","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/default_tune_control.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Tuning Control Parameters — default_tune_control","text":"","code":"default_tune_control(   pilot_proposal_sd = 0.5,   pilot_n = 100,   pilot_m = 2000,   pilot_target_var = 1,   pilot_burn_in = 500,   pilot_reps = 100,   pilot_resample_algorithm = c(\"SISAR\", \"SISR\", \"SIS\"),   pilot_resample_fn = c(\"stratified\", \"systematic\", \"multinomial\") )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/default_tune_control.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Tuning Control Parameters — default_tune_control","text":"pilot_proposal_sd Standard deviation pilot proposals. Default 0.5. pilot_n Number pilot particles particle filter. Default 100. pilot_m Number iterations MCMC. Default 2000. pilot_target_var target variance posterior log-likelihood evaluated estimated posterior mean. Default 1. pilot_burn_in Number burn-iterations MCMC. Default 500. pilot_reps Number times particle filter run. Default 100. pilot_resample_algorithm resample_algorithm used pilot particle filter. Default \"SISAR\". pilot_resample_fn resampling function used pilot particle filter. Default \"stratified\".","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/default_tune_control.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Tuning Control Parameters — default_tune_control","text":"list tuning control parameters.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/default_tune_control.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create Tuning Control Parameters — default_tune_control","text":"M. K. Pitt, R. d. S. Silva, P. Giordani, R. Kohn. properties Markov chain Monte Carlo simulation methods based particle filter. Journal Econometrics, 171(2):134–151, 2012. doi: https://doi.org/10.1016/j.jeconom.2012.06.004 J. Dahlin T. B. Schön. Getting started particle Metropolis-Hastings inference nonlinear dynamical models. Journal Statistical Software, 88(2):1–41, 2019. doi: 10.18637/jss.v088.c02","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-back_transform_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to back-transform parameters — .back_transform_params","title":"Internal function to back-transform parameters — .back_transform_params","text":"Internal function back-transform parameters","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-back_transform_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to back-transform parameters — .back_transform_params","text":"","code":".back_transform_params(theta_trans, transform)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-back_transform_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function to back-transform parameters — .back_transform_params","text":"theta_trans transformed parameter vector transform transformation type parameter","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-back_transform_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function to back-transform parameters — .back_transform_params","text":"original parameter vector","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-check_params_match.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper function to validate input of user-defined functions and priors — .check_params_match","title":"Helper function to validate input of user-defined functions and priors — .check_params_match","text":"Helper function validate input user-defined functions priors","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-check_params_match.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper function to validate input of user-defined functions and priors — .check_params_match","text":"","code":".check_params_match(   init_fn,   transition_fn,   log_likelihood_fn,   pilot_init_params,   log_priors )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-check_params_match.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper function to validate input of user-defined functions and priors — .check_params_match","text":"init_fn function initialize state-space model. transition_fn function defines state transition state-space model. log_likelihood_fn function calculates log-likelihood state-space model given latent states. pilot_init_params vector initial parameter values. log_priors list functions computing log-prior parameter.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-compute_log_jacobian.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to compute the Jacobian of the transformation — .compute_log_jacobian","title":"Internal function to compute the Jacobian of the transformation — .compute_log_jacobian","text":"Internal function compute Jacobian transformation","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-compute_log_jacobian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to compute the Jacobian of the transformation — .compute_log_jacobian","text":"","code":".compute_log_jacobian(theta, transform)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-compute_log_jacobian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function to compute the Jacobian of the transformation — .compute_log_jacobian","text":"theta parameter vector (original scale) transform transformation type parameter","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-compute_log_jacobian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function to compute the Jacobian of the transformation — .compute_log_jacobian","text":"log-Jacobian transformation","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-ensure_dots.html","id":null,"dir":"Reference","previous_headings":"","what":"Ensure that a function has a `...` argument — .ensure_dots","title":"Ensure that a function has a `...` argument — .ensure_dots","text":"Ensure function `...` argument","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-ensure_dots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ensure that a function has a `...` argument — .ensure_dots","text":"","code":".ensure_dots(fun)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-ensure_dots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ensure that a function has a `...` argument — .ensure_dots","text":"fun function modify","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-ensure_dots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ensure that a function has a `...` argument — .ensure_dots","text":"modified function `...` added formals already present.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-particle_filter_core.html","id":null,"dir":"Reference","previous_headings":"","what":"Core Particle Filter Function — .particle_filter_core","title":"Core Particle Filter Function — .particle_filter_core","text":"function implements underlying logic used particle filters state space model using sequential Monte Carlo methods.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-particle_filter_core.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core Particle Filter Function — .particle_filter_core","text":"","code":".particle_filter_core(   y,   num_particles,   init_fn,   transition_fn,   weight_fn,   aux_weight_fn = NULL,   move_fn = NULL,   obs_times = NULL,   algorithm = c(\"BPF\", \"APF\", \"RMPF\"),   resample_algorithm = c(\"SIS\", \"SISR\", \"SISAR\"),   resample_fn = c(\"stratified\", \"systematic\", \"multinomial\"),   threshold = NULL,   return_particles = TRUE,   ... )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-particle_filter_core.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core Particle Filter Function — .particle_filter_core","text":"y numeric vector matrix observations. row represents observation time step. observations equally spaced, use obs_times argument. num_particles positive integer specifying number particles. init_fn function initialize particles. take `num_particles` return matrix vector initial states. Additional model parameters can passed via .... transition_fn function propagating particles. take `particles` optionally `t`. Additional model parameters via .... weight_fn function computes log weights particles given observations current particles. take `y`, `particles`, `t` arguments. function can include model-specific parameters named arguments. obs_times numeric vector specifying observation time points. Must match number rows y, defaults 1:nrow(y). resample_algorithm character string specifying filtering resample algorithm: \"SIS\" resampling, \"SISR\" resampling every time step, \"SISAR\" adaptive resampling ESS drops threshold. Using \"SISR\" \"SISAR\" avoid weight degeneracy recommended. Default \"SISAR\". resample_fn string indicating resampling method: \"stratified\", \"systematic\", \"multinomial\". Default \"stratified\". threshold numeric value specifying ESS threshold \"SISAR\". Defaults num_particles / 2. return_particles Logical; TRUE, returns full particle weight histories. ... Additional arguments passed init_fn, transition_fn, log_likelihood_fn.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-particle_filter_core.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core Particle Filter Function — .particle_filter_core","text":"list components: state_est Estimated states time (weighted mean particles). ess Effective sample size time step. loglike Total log-likelihood. loglike_history Log-likelihood time step. algorithm filtering algorithm used. particles_history Matrix particle states time   (return_particles = TRUE). weights_history Matrix particle weights time   (return_particles = TRUE).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-particle_filter_core.html","id":"model-specification","dir":"Reference","previous_headings":"","what":"Model Specification","title":"Core Particle Filter Function — .particle_filter_core","text":"Particle filter implementations package assume discrete-time state-space model defined : sequence latent states \\(x_0, x_1, \\ldots, x_T\\) evolving   according Markov process. Observations \\(y_1, \\ldots, y_T\\) conditionally   independent given corresponding latent states. model specified : $$x_0 \\sim \\mu_\\theta$$ $$x_t \\sim f_\\theta(x_t \\mid x_{t-1}), \\quad t = 1, \\ldots, T$$ $$y_t \\sim g_\\theta(y_t \\mid x_t), \\quad t = 1, \\ldots, T$$ \\(\\theta\\) denotes model parameters passed via .... user provides following functions: init_fn: draws initial distribution   \\(\\mu_\\theta\\). transition_fn: generates evaluates transition   density \\(f_\\theta\\). weight_fn: evaluates observation likelihood   \\(g_\\theta\\).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-pilot_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Pilot Run for Particle Filter Tuning — .pilot_run","title":"Pilot Run for Particle Filter Tuning — .pilot_run","text":"internal function repeatedly evaluates particle filter order estimate variance log-likelihoods compute recommended target number particles Particle Marginal Metropolis Hastings (PMMH) algorithm.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-pilot_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pilot Run for Particle Filter Tuning — .pilot_run","text":"","code":".pilot_run(   pf_wrapper,   y,   pilot_n,   pilot_reps,   init_fn,   transition_fn,   log_likelihood_fn,   obs_times = NULL,   resample_fn = NULL,   ... )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-pilot_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pilot Run for Particle Filter Tuning — .pilot_run","text":"pilot_n integer specifying initial number particles use. pilot_reps integer specifying number repetitions pilot run.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-pilot_run.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pilot Run for Particle Filter Tuning — .pilot_run","text":"list containing: variance_estimate estimated variance log-likelihoods   pilot run. target_N number particles used PMMH algorithm. pilot_loglikes numeric vector log-likelihood values computed   run.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-pilot_run.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pilot Run for Particle Filter Tuning — .pilot_run","text":"function performs pilot_reps evaluations particle filter using provided parameter vector theta. estimates variance log-likelihoods scales initial particle number variance. final number particles taken ceiling scaled value minimum 50 maximum 1000.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-run_pilot_chain.html","id":null,"dir":"Reference","previous_headings":"","what":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","title":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","text":"Run Pilot Chain Posterior Estimation","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-run_pilot_chain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","text":"","code":".run_pilot_chain(   pf_wrapper,   y,   pilot_m,   pilot_n,   pilot_reps,   init_fn,   transition_fn,   log_likelihood_fn,   log_priors,   proposal_sd,   obs_times = NULL,   param_transform = NULL,   pilot_init_params = NULL,   verbose = FALSE,   ... )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-run_pilot_chain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","text":"pilot_m integer specifying number iterations pilot chain. pilot_n integer specifying number particles particle filter. pilot_reps integer specifying number repetitions pilot run. log_priors list functions representing log-priors model parameter. proposal_sd numeric vector specifying standard deviations random walk proposal distribution parameter. param_transform character vector specifying parameter transformations proposing parameters using random walk. Currently supports \"log\" log-transformation, \"logit\" logit transformation, \"identity\" transformation. Default `NULL`, correspond transformation (\"identity). pilot_init_params numeric vector initial parameter values. `NULL`, default vector ones. Default `NULL`. ... Additional arguments passed particle filter function.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-run_pilot_chain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","text":"list containing: pilot_theta_mean numeric vector posterior mean parameters. pilot_theta_cov matrix posterior covariance (variance one parameter). target_N estimated target number particles PMMH algorithm. pilot_theta_chain matrix containing chain parameter values throughout pilot run. pilot_loglike_chain vector containing log-likelihood values associated iteration pilot chain.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-run_pilot_chain.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","text":"function runs pilot chain estimate posterior mean covariance model parameters using particle filter. chain run `pilot_m` iterations, iteration proposing new parameters evaluating likelihood prior. chain used estimate posterior mean covariance, used tune number particles Particle Marginal Metropolis Hastings (PMMH) algorithm.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-transform_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to transform parameters — .transform_params","title":"Internal function to transform parameters — .transform_params","text":"Internal function transform parameters","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-transform_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to transform parameters — .transform_params","text":"","code":".transform_params(theta, transform)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-transform_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function to transform parameters — .transform_params","text":"theta parameter vector transform transformation type parameter","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-transform_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function to transform parameters — .transform_params","text":"transformed parameter vector","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ess.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate effective sample size (ESS) of MCMC chains. — ess","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"Estimate effective sample size (ESS) MCMC chains.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ess.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"","code":"ess(chains)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ess.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"chains matrix (iterations x chains) data.frame 'chain' column parameter columns.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ess.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"estimated effective sample size (ESS) given matrix, named vector ESS values given data frame.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ess.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"Uses formula ESS proposed Vehtari et al. (2021).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ess.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"Vehtari et al. (2021). Rank-normalization, folding, localization: improved R-hat assessing convergence MCMC. Available : https://doi.org/10.1214/20-BA1221","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ess.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"","code":"# With a matrix: chains <- matrix(rnorm(3000), nrow = 1000, ncol = 3) ess(chains) #> [1] 2752.906  # With a data frame: chains_df <- data.frame(   chain = rep(1:3, each = 1000),   param1 = rnorm(3000),   param2 = rnorm(3000) ) ess(chains_df) #>   param1   param2  #> 2910.196 3000.000"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter.html","id":null,"dir":"Reference","previous_headings":"","what":"Particle filter functions — particle_filter","title":"Particle filter functions — particle_filter","text":"package provides several particle filter implementations state-space models estimating intractable marginal likelihood \\(p(y_{1:T}\\mid \\theta)\\): auxiliary_filter bootstrap_filter resample_move_filter simplest one bootstrap_filter, thus recommended starting point.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter_common_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Common Parameters for Particle Filters — particle_filter_common_params","title":"Common Parameters for Particle Filters — particle_filter_common_params","text":"parameters shared particle filter implementations bootstrap filter, auxiliary particle filter, resample-move particle filter.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter_common_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Common Parameters for Particle Filters — particle_filter_common_params","text":"y numeric vector matrix observations. row represents observation time step. observations equally spaced, use obs_times argument. num_particles positive integer specifying number particles. init_fn function initialize particles. take `num_particles` return matrix vector initial states. Additional model parameters can passed via .... transition_fn function propagating particles. take `particles` optionally `t`. Additional model parameters via .... log_likelihood_fn function returns log-likelihood particle given current observation, particles, optionally `t`. Additional parameters via .... obs_times numeric vector specifying observation time points. Must match number rows y, defaults 1:nrow(y). resample_algorithm character string specifying filtering resample algorithm: \"SIS\" resampling, \"SISR\" resampling every time step, \"SISAR\" adaptive resampling ESS drops threshold. Using \"SISR\" \"SISAR\" avoid weight degeneracy recommended. Default \"SISAR\". resample_fn string indicating resampling method: \"stratified\", \"systematic\", \"multinomial\". Default \"stratified\". threshold numeric value specifying ESS threshold \"SISAR\". Defaults num_particles / 2. return_particles Logical; TRUE, returns full particle weight histories. ... Additional arguments passed init_fn, transition_fn, log_likelihood_fn.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter_model_specification.html","id":null,"dir":"Reference","previous_headings":"","what":"Model Specification for Particle Filters — particle_filter_model_specification","title":"Model Specification for Particle Filters — particle_filter_model_specification","text":"Model Specification Particle Filters","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter_model_specification.html","id":"model-specification","dir":"Reference","previous_headings":"","what":"Model Specification","title":"Model Specification for Particle Filters — particle_filter_model_specification","text":"Particle filter implementations package assume discrete-time state-space model defined : sequence latent states \\(x_0, x_1, \\ldots, x_T\\) evolving   according Markov process. Observations \\(y_1, \\ldots, y_T\\) conditionally   independent given corresponding latent states. model specified : $$x_0 \\sim \\mu_\\theta$$ $$x_t \\sim f_\\theta(x_t \\mid x_{t-1}), \\quad t = 1, \\ldots, T$$ $$y_t \\sim g_\\theta(y_t \\mid x_t), \\quad t = 1, \\ldots, T$$ \\(\\theta\\) denotes model parameters passed via .... user provides following functions: init_fn: draws initial distribution   \\(\\mu_\\theta\\). transition_fn: generates evaluates transition   density \\(f_\\theta\\). weight_fn: evaluates observation likelihood   \\(g_\\theta\\).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter_returns.html","id":null,"dir":"Reference","previous_headings":"","what":"Shared Return Values for Particle Filters — particle_filter_returns","title":"Shared Return Values for Particle Filters — particle_filter_returns","text":"block documents common return value particle filtering functions.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter_returns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shared Return Values for Particle Filters — particle_filter_returns","text":"list components: state_est Estimated states time (weighted mean particles). ess Effective sample size time step. loglike Total log-likelihood. loglike_history Log-likelihood time step. algorithm filtering algorithm used. particles_history Matrix particle states time   (return_particles = TRUE). weights_history Matrix particle weights time   (return_particles = TRUE).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":null,"dir":"Reference","previous_headings":"","what":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"function implements Particle Marginal Metropolis-Hastings (PMMH) resample_algorithm perform Bayesian inference state-space models. first runs pilot chain tune proposal distribution number particles particle filter, runs main PMMH chain.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"","code":"pmmh(   pf_wrapper,   y,   m,   init_fn,   transition_fn,   log_likelihood_fn,   log_priors,   pilot_init_params,   burn_in,   num_chains = 4,   obs_times = NULL,   resample_algorithm = c(\"SISAR\", \"SISR\", \"SIS\"),   resample_fn = c(\"stratified\", \"systematic\", \"multinomial\"),   param_transform = NULL,   tune_control = default_tune_control(),   verbose = FALSE,   return_latent_state_est = FALSE,   seed = NULL,   num_cores = 1,   ... )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"pf_wrapper particle filter wrapper function. See particle_filter particle filters implemented package. y numeric vector matrix observations. row represents observation time step. observations equally spaced, use obs_times argument. m integer specifying number MCMC iterations chain. init_fn function initialize particles. take `num_particles` return matrix vector initial states. Additional model parameters can passed via .... transition_fn function propagating particles. take `particles` optionally `t`. Additional model parameters via .... log_likelihood_fn function returns log-likelihood particle given current observation, particles, optionally `t`. Additional parameters via .... log_priors list functions computing log-prior parameter. pilot_init_params list initial parameter values. list length num_chains element named vector initial parameter values. burn_in integer indicating number initial MCMC iterations discard burn-. num_chains integer specifying number PMMH chains run. obs_times numeric vector specifying observation time points. Must match number rows y, defaults 1:nrow(y). resample_algorithm character string specifying resampling algorithm use particle filter. Options : #' SIS: Sequential Importance Sampling (without resampling). SISR: Sequential Importance Sampling resampling   every time step. SISAR: SIS adaptive resampling based Effective   Sample Size (ESS). Resampling triggered ESS falls   given threshold (default particles / 2). Can modified   specifying threshold argument (...), see also   particle_filter. resample_fn string indicating resampling method: \"stratified\", \"systematic\", \"multinomial\". Default \"stratified\". param_transform optional character vector specifies transformation applied parameter proposing. proposal made using multivariate normal distribution transformed scale. Parameters mapped back original scale evaluation. Currently supports \"log\", \"logit\", \"identity\". NULL, \"identity\" transformation used parameters. tune_control list pilot tuning controls (e.g., pilot_m, pilot_reps). See default_tune_control. verbose logical value indicating whether print information pilot_run tuning. Defaults FALSE. return_latent_state_est logical value indicating whether return latent state estimates time step. Defaults FALSE. seed optional integer set seed reproducibility. num_cores integer specifying number cores use parallel processing. Defaults 1. chain assigned core, number cores exceed number chains (num_chains). progress information given user limited using one core. ... Additional arguments passed init_fn, transition_fn, log_likelihood_fn.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"list containing: theta_chain dataframe post burn-parameter samples. latent_state_chain return_latent_state_est   TRUE, list matrices containing latent state estimates   time step. diagnostics Diagnostics containing ESS Rhat   parameter (see ess rhat   documentation).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"PMMH resample_algorithm essentially Metropolis Hastings algorithm, instead using intractable marginal likelihood \\(p(y_{1:T}\\mid \\theta)\\) instead uses estimated likelihood using particle filter (see particle_filter available particle filters). Values proposed using multivariate normal distribution transformed space (specified using `param_transform`). proposal covariance number particles chosen based pilot run. number particles chosen variance log-likelihood estimate estimated posterior mean approximately 1 (minimum 50 particles maximum 1000).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"Andrieu et al. (2010). Particle Markov chain Monte Carlo methods. Journal Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342. doi: 10.1111/j.1467-9868.2009.00736.x","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"","code":"init_fn <- function(num_particles) {   rnorm(num_particles, mean = 0, sd = 1) } transition_fn <- function(particles, phi, sigma_x) {   phi * particles + sin(particles) +     rnorm(length(particles), mean = 0, sd = sigma_x) } log_likelihood_fn <- function(y, particles, sigma_y) {   dnorm(y, mean = cos(particles), sd = sigma_y, log = TRUE) } log_prior_phi <- function(phi) {   dnorm(phi, mean = 0, sd = 1, log = TRUE) } log_prior_sigma_x <- function(sigma) {   dexp(sigma, rate = 1, log = TRUE) } log_prior_sigma_y <- function(sigma) {   dexp(sigma, rate = 1, log = TRUE) } log_priors <- list(   phi = log_prior_phi,   sigma_x = log_prior_sigma_x,   sigma_y = log_prior_sigma_y ) # Generate data t_val <- 10 x <- numeric(t_val) y <- numeric(t_val) phi <- 0.8 sigma_x <- 1 sigma_y <- 0.5  init_state <- rnorm(1, mean = 0, sd = 1) x[1] <- phi * init_state + sin(init_state) + rnorm(1, mean = 0, sd = sigma_x) y[1] <- x[1] + rnorm(1, mean = 0, sd = sigma_y) for (t in 2:t_val) {   x[t] <- phi * x[t - 1] + sin(x[t - 1]) + rnorm(1, mean = 0, sd = sigma_x)   y[t] <- cos(x[t]) + rnorm(1, mean = 0, sd = sigma_y) } x <- c(init_state, x)  # Should use higher MCMC iterations in practice (m) pmmh_result <- pmmh(   pf_wrapper = bootstrap_filter,   y = y,   m = 1000,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   log_priors = log_priors,   pilot_init_params = list(     c(phi = 0.8, sigma_x = 1, sigma_y = 0.5),     c(phi = 1, sigma_x = 0.5, sigma_y = 1)   ),   burn_in = 100,   num_chains = 2,   param_transform = list(     phi = \"identity\",     sigma_x = \"log\",     sigma_y = \"log\"   ),   tune_control = default_tune_control(pilot_m = 500, pilot_burn_in = 100) ) #> Running chain 1... #> Running pilot chain for tuning... #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> Running chain 2... #> Running pilot chain for tuning... #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> PMMH Results Summary: #>  Parameter Mean   SD Median  2.5% 97.5% ESS  Rhat #>        phi 0.40 1.15   0.69 -2.30  2.61   8 1.103 #>    sigma_x 1.25 1.20   0.86  0.00  4.28  66 1.050 #>    sigma_y 1.08 0.33   1.02  0.61  1.86 116 1.001 #> Warning: Some ESS values are below 400, indicating poor mixing. Consider running the chains for more iterations. #> Warning:  #> Some Rhat values are above 1.01, indicating that the chains have not converged.  #> Consider running the chains for more iterations and/or increase burn_in. # Convergence warning is expected with such low MCMC iterations.  # Suppose we have data for t=1,2,3,5,6,7,8,9,10 (i.e., missing at t=4)  obs_times <- c(1, 2, 3, 5, 6, 7, 8, 9, 10) y <- y[obs_times]  # Specify observation times in the pmmh using obs_times pmmh_result <- pmmh(   pf_wrapper = bootstrap_filter,   y = y,   m = 1000,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   log_priors = log_priors,   pilot_init_params = list(     c(phi = 0.8, sigma_x = 1, sigma_y = 0.5),     c(phi = 1, sigma_x = 0.5, sigma_y = 1)   ),   burn_in = 100,   num_chains = 2,   obs_times = obs_times,   param_transform = list(     phi = \"identity\",     sigma_x = \"log\",     sigma_y = \"log\"   ),   tune_control = default_tune_control(pilot_m = 500, pilot_burn_in = 100) ) #> Running chain 1... #> Running pilot chain for tuning... #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> Running chain 2... #> Running pilot chain for tuning... #> Using 50 particles for PMMH: #> Running Particle MCMC chain with tuned settings... #> PMMH Results Summary: #>  Parameter Mean   SD Median  2.5% 97.5% ESS  Rhat #>        phi 0.43 1.04   0.73 -1.89  1.93  20 1.106 #>    sigma_x 1.20 1.02   0.88  0.08  3.98 125 1.020 #>    sigma_y 1.08 0.33   1.02  0.63  1.87 151 1.007 #> Warning: Some ESS values are below 400, indicating poor mixing. Consider running the chains for more iterations. #> Warning:  #> Some Rhat values are above 1.01, indicating that the chains have not converged.  #> Consider running the chains for more iterations and/or increase burn_in."},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/print.pmmh_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for PMMH output — print.pmmh_output","title":"Print method for PMMH output — print.pmmh_output","text":"Displays concise summary parameter estimates PMMH output object, including means, standard deviations, medians, 95% credible intervals, effective sample sizes (ESS), Rhat. provides quick overview posterior distribution convergence diagnostics.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/print.pmmh_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for PMMH output — print.pmmh_output","text":"","code":"# S3 method for class 'pmmh_output' print(x, ...)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/print.pmmh_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for PMMH output — print.pmmh_output","text":"x object class `pmmh_output`. ... Additional arguments.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/print.pmmh_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for PMMH output — print.pmmh_output","text":"object `x` invisibly.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/print.pmmh_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print method for PMMH output — print.pmmh_output","text":"","code":"# Create dummy chains for two parameters across two chains chain1 <- data.frame(param1 = rnorm(100), param2 = rnorm(100), chain = 1) chain2 <- data.frame(param1 = rnorm(100), param2 = rnorm(100), chain = 2) dummy_output <- list(   theta_chain = rbind(chain1, chain2),   diagnostics = list(     ess = c(param1 = 200, param2 = 190),     rhat = c(param1 = 1.01, param2 = 1.00)   ) ) class(dummy_output) <- \"pmmh_output\" print(dummy_output) #> PMMH Results Summary: #>  Parameter Mean   SD Median  2.5% 97.5% ESS Rhat #>     param1 0.10 1.03   0.09 -1.88  1.93 200 1.01 #>     param2 0.04 0.94   0.02 -1.71  2.08 190 1.00"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/resample_move_filter.html","id":null,"dir":"Reference","previous_headings":"","what":"Resample-Move Particle Filter (RMPF) — resample_move_filter","title":"Resample-Move Particle Filter (RMPF) — resample_move_filter","text":"Resample-Move Particle Filter differs standard resampling methods including Metropolis–Hastings move step resampling. additional step can increase particle diversity , contexts, help mitigate sample impoverishment.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/resample_move_filter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resample-Move Particle Filter (RMPF) — resample_move_filter","text":"","code":"resample_move_filter(   y,   num_particles,   init_fn,   transition_fn,   log_likelihood_fn,   move_fn,   obs_times = NULL,   resample_fn = c(\"stratified\", \"systematic\", \"multinomial\"),   return_particles = TRUE,   ... )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/resample_move_filter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resample-Move Particle Filter (RMPF) — resample_move_filter","text":"y numeric vector matrix observations. row represents observation time step. observations equally spaced, use obs_times argument. num_particles positive integer specifying number particles. init_fn function initialize particles. take `num_particles` return matrix vector initial states. Additional model parameters can passed via .... transition_fn function propagating particles. take `particles` optionally `t`. Additional model parameters via .... log_likelihood_fn function returns log-likelihood particle given current observation, particles, optionally `t`. Additional parameters via .... move_fn function moves resampled particles. Takes `particles`, optionally `t`, returns updated particles. Can use ... model-specific arguments. obs_times numeric vector specifying observation time points. Must match number rows y, defaults 1:nrow(y). resample_fn string indicating resampling method: \"stratified\", \"systematic\", \"multinomial\". Default \"stratified\". return_particles Logical; TRUE, returns full particle weight histories. ... Additional arguments passed init_fn, transition_fn, log_likelihood_fn.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/resample_move_filter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resample-Move Particle Filter (RMPF) — resample_move_filter","text":"list components: state_est Estimated states time (weighted mean particles). ess Effective sample size time step. loglike Total log-likelihood. loglike_history Log-likelihood time step. algorithm filtering algorithm used. particles_history Matrix particle states time   (return_particles = TRUE). weights_history Matrix particle weights time   (return_particles = TRUE).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/resample_move_filter.html","id":"the-resample-move-particle-filter-rmpf-","dir":"Reference","previous_headings":"","what":"The Resample-Move Particle Filter (RMPF)","title":"Resample-Move Particle Filter (RMPF) — resample_move_filter","text":"Resample-Move Particle Filter enhances standard particle filtering framework introducing move step resampling. resampling time \\(t\\), particles \\(\\{x_t^{()}\\}_{=1}^N\\) propagated via Markov kernel \\(K_t(x' \\mid x)\\) leaves target posterior \\(p(x_t \\mid y_{1:t})\\) invariant: $$   x_t^{()} \\sim K_t(\\cdot \\mid x_t^{()}). $$ move step often uses Metropolis-Hastings update preserves posterior distribution invariant distribution \\(K_t\\). goal move step mitigate particle impoverishment — collapse diversity caused resampling selecting unique particles — rejuvenating particles exploring state space thoroughly. leads improved approximation filtering distribution reduces Monte Carlo error. move_fn argument represents transition kernel take current particle set input return updated particles. Additional model-specific parameters may passed via .... Default resampling method stratified resampling, lower variance multinomial resampling (Douc et al., 2005). implementation, resampling performed every time step using specified method (default: stratified), followed immediately move step. follows standard Resample-Move framework described Gilks Berzuini (2001). Unlike particle filtering variants may use ESS threshold decide whether resample, RMPF requires resampling every step ensure effectiveness subsequent rejuvenation step.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/resample_move_filter.html","id":"model-specification","dir":"Reference","previous_headings":"","what":"Model Specification","title":"Resample-Move Particle Filter (RMPF) — resample_move_filter","text":"Particle filter implementations package assume discrete-time state-space model defined : sequence latent states \\(x_0, x_1, \\ldots, x_T\\) evolving   according Markov process. Observations \\(y_1, \\ldots, y_T\\) conditionally   independent given corresponding latent states. model specified : $$x_0 \\sim \\mu_\\theta$$ $$x_t \\sim f_\\theta(x_t \\mid x_{t-1}), \\quad t = 1, \\ldots, T$$ $$y_t \\sim g_\\theta(y_t \\mid x_t), \\quad t = 1, \\ldots, T$$ \\(\\theta\\) denotes model parameters passed via .... user provides following functions: init_fn: draws initial distribution   \\(\\mu_\\theta\\). transition_fn: generates evaluates transition   density \\(f_\\theta\\). weight_fn: evaluates observation likelihood   \\(g_\\theta\\).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/resample_move_filter.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Resample-Move Particle Filter (RMPF) — resample_move_filter","text":"Gilks, W. R., & Berzuini, C. (2001). Following moving target—Monte Carlo inference dynamic Bayesian models. Journal Royal Statistical Society: Series B (Statistical Methodology), 63(1), 127–146. doi:10.2307/2670179 Douc, R., Cappé, O., & Moulines, E. (2005). Comparison Resampling Schemes Particle Filtering. Accessible : https://arxiv.org/abs/cs/0507025","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/resample_move_filter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Resample-Move Particle Filter (RMPF) — resample_move_filter","text":"","code":"init_fn <- function(num_particles) rnorm(num_particles, 0, 1) transition_fn <- function(particles) particles + rnorm(length(particles)) log_likelihood_fn <- function(y, particles) {   dnorm(y, mean = particles, sd = 1, log = TRUE) }  # Define a simple random-walk Metropolis move function move_fn <- function(particle, y) {   proposal <- particle + rnorm(1, 0, 0.1)   log_p_current <- log_likelihood_fn(y = y, particles = particle)   log_p_proposal <- log_likelihood_fn(y = y, particles = proposal)   if (log(runif(1)) < (log_p_proposal - log_p_current)) {     return(proposal)   } else {     return(particle)   } }  y <- cumsum(rnorm(50)) # Dummy data num_particles <- 100  result <- resample_move_filter(   y = y,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   move_fn = move_fn ) plot(result$state_est,   type = \"l\", col = \"blue\", main = \"RMPF State Estimates\",   ylim = range(c(result$state_est, y)) ) points(y, col = \"red\", pch = 20)    # With parameters init_fn <- function(num_particles) rnorm(num_particles, 0, 1) transition_fn <- function(particles, mu) {   particles + rnorm(length(particles), mean = mu) } log_likelihood_fn <- function(y, particles, sigma) {   dnorm(y, mean = particles, sd = sigma, log = TRUE) } move_fn <- function(particle, y, sigma) {   proposal <- particle + rnorm(1, 0, 0.1)   log_p_curr <- log_likelihood_fn(y = y, particles = particle, sigma = sigma)   log_p_prop <- log_likelihood_fn(y = y, particles = proposal, sigma = sigma)   if (log(runif(1)) < (log_p_prop - log_p_curr)) {     return(proposal)   } else {     return(particle)   } }  y <- cumsum(rnorm(50)) num_particles <- 100  result <- resample_move_filter(   y = y,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   move_fn = move_fn,   mu = 1,   sigma = 1 ) plot(result$state_est,   type = \"l\", col = \"blue\", main = \"RMPF with Parameters\",   ylim = range(c(result$state_est, y)) ) points(y, col = \"red\", pch = 20)    # With observation gaps simulate_ssm <- function(num_steps, mu, sigma) {   x <- numeric(num_steps)   y <- numeric(num_steps)   x[1] <- rnorm(1, mean = 0, sd = sigma)   y[1] <- rnorm(1, mean = x[1], sd = sigma)   for (t in 2:num_steps) {     x[t] <- mu * x[t - 1] + sin(x[t - 1]) + rnorm(1, mean = 0, sd = sigma)     y[t] <- x[t] + rnorm(1, mean = 0, sd = sigma)   }   y }  data <- simulate_ssm(10, mu = 1, sigma = 1) obs_times <- c(1, 2, 3, 5, 6, 7, 8, 9, 10) # skip t=4 data_obs <- data[obs_times]  init_fn <- function(num_particles) rnorm(num_particles, 0, 1) transition_fn <- function(particles, mu) {   particles + rnorm(length(particles), mean = mu) } log_likelihood_fn <- function(y, particles, sigma) {   dnorm(y, mean = particles, sd = sigma, log = TRUE) } move_fn <- function(particle, y, sigma) {   proposal <- particle + rnorm(1, 0, 0.1)   log_p_cur <- log_likelihood_fn(y = y, particles = particle, sigma = sigma)   log_p_prop <- log_likelihood_fn(y = y, particles = proposal, sigma = sigma)   if (log(runif(1)) < (log_p_prop - log_p_cur)) {     return(proposal)   } else {     return(particle)   } }  result <- resample_move_filter(   y = data_obs,   num_particles = 100,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   move_fn = move_fn,   obs_times = obs_times,   mu = 1,   sigma = 1 ) plot(result$state_est,   type = \"l\", col = \"blue\", main = \"RMPF with Observation Gaps\",   ylim = range(c(result$state_est, data)) ) points(data_obs, col = \"red\", pch = 20)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute split Rhat statistic — rhat","title":"Compute split Rhat statistic — rhat","text":"Compute split Rhat statistic","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute split Rhat statistic — rhat","text":"","code":"rhat(chains)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute split Rhat statistic — rhat","text":"chains matrix (iterations x chains) data.frame 'chain' column parameter columns.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute split Rhat statistic — rhat","text":"Rhat value (matrix input) named vector Rhat values.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute split Rhat statistic — rhat","text":"Uses formula split-Rhat proposed Gelman et al. (2013).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute split Rhat statistic — rhat","text":"Gelman et al. (2013). Bayesian Data Analysis, 3rd Edition.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute split Rhat statistic — rhat","text":"","code":"# Example with matrix chains <- matrix(rnorm(3000), nrow = 1000, ncol = 3) rhat(chains) #> [1] 1.000749 #' # Example with data frame chains_df <- data.frame(   chain = rep(1:3, each = 1000),   param1 = rnorm(3000),   param2 = rnorm(3000) ) rhat(chains_df) #> param1 param2  #>      1      1"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/summary.pmmh_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary method for PMMH output — summary.pmmh_output","title":"Summary method for PMMH output — summary.pmmh_output","text":"function returns summary statistics PMMH output objects, including means, standard deviations, medians, credible intervals, diagnostics.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/summary.pmmh_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary method for PMMH output — summary.pmmh_output","text":"","code":"# S3 method for class 'pmmh_output' summary(object, ...)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/summary.pmmh_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary method for PMMH output — summary.pmmh_output","text":"object object class `pmmh_output`. ... Additional arguments.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/summary.pmmh_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary method for PMMH output — summary.pmmh_output","text":"data frame containing summary statistics parameter.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/summary.pmmh_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary method for PMMH output — summary.pmmh_output","text":"","code":"# Create dummy chains for two parameters across two chains chain1 <- data.frame(param1 = rnorm(100), param2 = rnorm(100), chain = 1) chain2 <- data.frame(param1 = rnorm(100), param2 = rnorm(100), chain = 2) dummy_output <- list(   theta_chain = rbind(chain1, chain2),   diagnostics = list(     ess = c(param1 = 200, param2 = 190),     rhat = c(param1 = 1.01, param2 = 1.00)   ) ) class(dummy_output) <- \"pmmh_output\" summary(dummy_output) #>               mean        sd      median      2.5%    97.5% ESS Rhat #> param1  0.01499914 1.0339426 -0.07112074 -1.923310 2.193800 200 1.01 #> param2 -0.06911637 0.9336208 -0.09527880 -1.661481 1.571068 190 1.00"},{"path":"https://bjarkehautop.github.io/bayesSSM/news/index.html","id":"bayesssm-development-version","dir":"Changelog","previous_headings":"","what":"bayesSSM (development version)","title":"bayesSSM (development version)","text":"Small improvements README vignettes clarity.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/news/index.html","id":"bayesssm-070","dir":"Changelog","previous_headings":"","what":"bayesSSM 0.7.0","title":"bayesSSM 0.7.0","text":"CRAN release: 2025-08-26 Add auxiliary_filter resample_move_filter auxiliary resample-move particle filters. See supported filters ?particle_filter. Refactor pmmh internals support multiple particle filters. Add pf_wrapper argument pmmh specify particle filter use. Improve argument checking switching checkmate package, providing clearer consistent error messages. Make minor edits README vignettes clarity.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/news/index.html","id":"bayesssm-061","dir":"Changelog","previous_headings":"","what":"bayesSSM 0.6.1","title":"bayesSSM 0.6.1","text":"CRAN release: 2025-06-23 Improved reproducibility pmmh: Setting seed now ensures consistent results regardless number cores used. Improvement performance pmmh slightly. Rewrote resampling step particle_filter C++ improved performance. Added new article detailed-overview goes theory, implementation details, tips using package effectively. Improved documentation package, including README articles, provide clearer explanations examples.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/news/index.html","id":"bayesssm-050","dir":"Changelog","previous_headings":"","what":"bayesSSM 0.5.0","title":"bayesSSM 0.5.0","text":"CRAN release: 2025-05-21 particles argument init_fn, passed particle_filter pmmh, deprecated. Please use num_particles instead. warning issued particles used. Added support time dependency functions. can now use t transition_fn likelihood_fn passing particle_filter pmmh. allows time-varying transition likelihood functions. Fixed bug particle_filter likelihood calculation causing shifted constant. Improved robustness pmmh encountering low log-likelihood values. Added scaling proposal covariance using \"logit\" pmmh. Improved documentation: updated package description, clarified text README vignette, added unit tests.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/news/index.html","id":"bayesssm-047","dir":"Changelog","previous_headings":"","what":"bayesSSM 0.4.7","title":"bayesSSM 0.4.7","text":"CRAN release: 2025-04-23 Initial CRAN submission.","code":""}]
