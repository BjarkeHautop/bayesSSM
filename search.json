[{"path":"https://bjarkehautop.github.io/bayesSSM/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 bayesSSM authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/sir-model.html","id":"sir-model","dir":"Articles","previous_headings":"","what":"SIR model","title":"sir-model","text":"implement following SIR model. Consider closed population size NN (.e. births, deaths migration) three compartments: susceptible, infected recovered. assume initially every individual susceptible except mm infected individuals. assume indivudlas infectious period, contacts given member population occur according time-homogeneous Poisson process rate λ/n\\lambda/n, λ>0\\lambda > 0 nn number susceptible. susceptible individual contacted, become infected instantaneously subsequently follow infectious process. let distribution infection period, II exponential distributed: ∼Exp(γ)\\sim \\text{Exp}(\\gamma) γ>0\\gamma > 0. Let S(t)S(t) denote number susceptible individuals (t)(t) denote number infectious individuals time t≥0t \\geq 0. Note, assumptions {(S(t),(t)):t≥0} \\{(S(t), (t)) : t \\geq 0\\}  Markov process, since infection event exponential distributed (since time next event Poisson process exponential) removal event, thus memoryless. infection removal events following rates: Infection Event: transition (s,)→(s−1,+1)   (s,) \\(s-1, +1)    occurs rate λnsi,   \\frac{\\lambda}{n}\\, s\\, ,    s>0s > 0 >0i > 0. Removal Event: transition (s,)→(s,−1)   (s,) \\(s, -1)    occurs rate γi,   \\gamma\\, ,    >0i > 0.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/sir-model.html","id":"partial-observations-and-noisy-measurements","dir":"Articles","previous_headings":"SIR model","what":"Partial observations and noisy measurements","title":"sir-model","text":"Suppose observe initial state number infectious individuals, (t)(t), discrete times t=0,1,…,Tt = 0, 1, \\ldots, T. assume true number infectious individuals latent state observe noisy version state, either higher lower (often lower). model Poisson distribution: Yt∣(t)∼Pois((t)), Y_t \\mid (t) \\sim \\operatorname{Pois}((t)),  ϕ>0\\phi > 0 overdispersion parameter.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/articles/sir-model.html","id":"simulate-data","dir":"Articles","previous_headings":"","what":"Simulate data","title":"sir-model","text":"simulate data SIR model following parameters: - t=0t=0 S(0)=90S(0) = 90, (0)=10I(0) = 10 R(0)=0R(0) = 0. - Infection rate λ=1.5\\lambda=1.5 removal rate γ=0.5\\gamma=0.5. - ϕ=3\\phi=3 overdispersion parameter. - observe number infectious individuals times t=0,1,…,10t=0, 1, \\ldots, 10. representing observations day. can simulate using fact two independent exponential distribution, event occurs rate sum rates. Now, generate data: plot :  interested performing Bayesian inference setup. define priors define priors λ,γ\\lambda, \\gamma ϕ\\phi λ∼half-N(1),γ∼half-N(2),1ϕ∼half-N(1),\\begin{align*}     \\lambda \\sim \\text{half-}N(1), \\\\     \\gamma \\sim \\text{half-}N(2), \\\\     \\frac{1}{\\sqrt{\\phi}} \\sim \\text{half-}N(1), \\end{align*} half-N()\\text{half-}N() denotes half-normal distribution mean 00 standard deviation aa. prior -dispersion parameter ϕ\\phi follows recommendation https://mc-stan.org/users/documentation/case-studies. define priors, since specify prior ϕ\\phi another scale need include Jacobian term likelihood. now define initial state, transition likelihood functions SIR model. Now can run PMMH algorithm estimate parameters. vignette use small number iterations (1000) 2 chains (also modify tuning use 100 iterations burn-10). practice, much higher. get convergence warnings expected, still recover true values. can access chains plot densities: λ\\lambda:  γ\\gamma:","code":"# --- Simulation settings and true parameters --- n_total <- 500 # Total population size init_infected <- 70 # Initially infectious individuals init_state <- c(n_total - init_infected, init_infected) # (s, i) at time 0 t_max <- 10 # Total number of days to simulate true_lambda <- 0.5 # True infection parameter true_gamma <- 0.2 # True removal parameter phi_true <- 1 # True dispersion parameter for negative binomial p_true <- 0.9 # True probability for binomial  # --- Functions for simulating the epidemic ---  # Gillespie simulation: simulate from current state for a time interval gillespie_step <- function(state, t_end, lambda, gamma, n_total) {   t <- 0   s <- state[1]   i <- state[2]   while (t < t_end && i > 0) {     rate_infection <- (lambda / n_total) * s * i     rate_removal <- gamma * i     rate_total <- rate_infection + rate_removal     if (rate_total <= 0) break     dt <- rexp(1, rate_total)     if (t + dt > t_end) break     t <- t + dt     # Decide which event occurs:     if (runif(1) < rate_infection / rate_total) {       # Infection event (only possible if s > 0)       if (s > 0) {         s <- s - 1         i <- i + 1       }     } else {       # Removal event       i <- i - 1     }   }   c(s, i) }  # Simulate epidemic trajectory using Gillespie simulate_epidemic_gillespie <- function(     n_total, init_infected, lambda, gamma, t_max) {   states <- matrix(0, nrow = t_max + 1, ncol = 2)   # initial state at t = 0   states[1, ] <- c(n_total - init_infected, init_infected)   state <- states[1, ]   for (t in 1:t_max) {     state <- gillespie_step(state, 1, lambda, gamma, n_total)     states[t + 1, ] <- state   }   states } # Simulate a \"true\" epidemic dataset true_states <- simulate_epidemic_gillespie(   n_total, init_infected, true_lambda, true_gamma, t_max ) latent_i <- true_states[, 2]  # Poisson observations <- rpois(length(latent_i), lambda = latent_i)  # Display simulated data: time, susceptible, latent infectious, observed counts print(data.frame(   time = 0:t_max, s = true_states[, 1], i = true_states[, 2], y = observations )) #>    time   s   i   y #> 1     0 430  70  83 #> 2     1 391  88  82 #> 3     2 348 113  97 #> 4     3 307 132 122 #> 5     4 266 147 138 #> 6     5 221 164 161 #> 7     6 183 171 166 #> 8     7 155 159 152 #> 9     8 137 143 151 #> 10    9 118 139 141 #> 11   10 107 121 122 # Function to create a tidy dataset for ggplot prepare_data_for_plot <- function(states, observations, t_max) {   # Organize the data into a tidy format   data <- data.frame(     time = 0:t_max,     s = states[, 1],     i = states[, 2],     y = observations   )    # Convert to long format for ggplot   data_long <- data %>%     gather(key = \"state\", value = \"count\", -time)    data_long }  # Function to plot the epidemic data plot_epidemic_data <- function(data_long, t_max) {   ggplot(data_long, aes(x = time, y = count, color = state)) +     geom_line(linewidth = 1.2) +     scale_color_manual(values = c(\"s\" = \"blue\", \"i\" = \"red\", \"y\" = \"green\")) +     labs(       x = \"Time (Days)\", y = \"Count\",       title = \"Susceptible, Infected, and Observed Counts\"     ) +     theme_minimal() +     theme(legend.title = element_blank()) +     scale_x_continuous(breaks = 0:t_max) +     theme(       axis.title = element_text(size = 12),       plot.title = element_text(size = 14, hjust = 0.5)     ) }  # Prepare data for plotting data_long <- prepare_data_for_plot(true_states, observations, t_max)  # Plot the results plot_epidemic_data(data_long, t_max) # Define the log-prior for the parameters log_prior_lambda <- function(lambda) {   extraDistr::dhnorm(lambda, sigma = 1, log = TRUE) }  log_prior_gamma <- function(gamma) {   extraDistr::dhnorm(gamma, sigma = 2, log = TRUE) }  log_prior_phi <- function(phi) {   if (phi <= 0) {     return(-Inf)   } # Ensure phi is positive    # Jacobian: |d(1/sqrt(phi))/dphi| = 1/(2 * phi^(3/2))   log_jacobian <- -log(2) - 1.5 * log(phi)   extraDistr::dhnorm(1 / sqrt(phi), sigma = 1, log = TRUE) + log_jacobian }  log_prior_p <- function(p) {   dunif(p, 0, 1, log = TRUE) }  log_priors <- list(   lambda = log_prior_lambda,   gamma = log_prior_gamma ) init_fn_epidemic <- function(particles) {   # Return a matrix with particles rows; each row is the initial state (s, i)   matrix(rep(init_state, each = particles), nrow = particles, byrow = FALSE) }  transition_fn_gillespie <- function(particles, lambda, gamma) {   new_particles <- t(apply(particles, 1, function(state) {     s <- state[1]     i <- state[2]     if (i == 0) {       return(c(s, i))     }     gillespie_step(state, t_end = 1, lambda, gamma, n_total)   }))   new_particles }  log_likelihood_fn_epidemic <- function(y, particles) {   # particles is expected to be a matrix with columns (s, i)   dpois(y, lambda = particles[, 2], log = TRUE) } result <- bayesSSM::pmmh(   y = observations,   m = 1000,   init_fn = init_fn_epidemic,   transition_fn = transition_fn_gillespie,   log_likelihood_fn = log_likelihood_fn_epidemic,   log_priors = log_priors,   init_params = c(lambda = 0.5, gamma = 0.5),   burn_in = 200,   num_chains = 2,   param_transform = list(lambda = \"log\", gamma = \"log\"),   tune_control = default_tune_control(pilot_m = 100, pilot_burn_in = 10),   verbose = TRUE,   seed = 1405, ) #> Running chain 1... #> Running pilot chain for tuning... #> Pilot chain posterior mean: #>    lambda     gamma  #> 0.5548529 0.1913232 #> Pilot chain posterior covariance (on transformed space): #>              lambda        gamma #> lambda 0.0008844233 1.541392e-04 #> gamma  0.0001541392 2.686372e-05 #> Using 100 particles for PMMH: #> Running particle MCMC chain with tuned settings... #> Running chain 2... #> Running pilot chain for tuning... #> Pilot chain posterior mean: #>    lambda     gamma  #> 0.4330822 0.1846308 #> Pilot chain posterior covariance (on transformed space): #>             lambda        gamma #> lambda 0.002879192 0.0014148984 #> gamma  0.001414898 0.0008559071 #> Using 100 particles for PMMH: #> Running particle MCMC chain with tuned settings... #> PMMH Results Summary: #>  Parameter Mean   SD Median CI.2.5% CI.97.5% ESS  Rhat #>     lambda 0.54 0.10   0.54    0.39     0.74  25 1.117 #>      gamma 0.20 0.02   0.19    0.16     0.25   3 1.051 #> Warning in bayesSSM::pmmh(y = observations, m = 1000, init_fn = #> init_fn_epidemic, : Some ESS values are below 400, indicating poor mixing. #> Consider running the chains for more iterations. #> Warning in bayesSSM::pmmh(y = observations, m = 1000, init_fn = init_fn_epidemic, :  #> Some Rhat values are above 1.01, indicating that the chains have not converged. Consider running the chains for more iterations and/or increase burn_in. chains <- result$theta_chain  chain_1 <- chains[[1]] chain_2 <- chains[[2]] lambda_chains <- cbind(   chain_1$lambda, chain_2$lambda )  ggplot2::ggplot() +   geom_density(aes(x = lambda_chains[, 1]), fill = \"blue\", alpha = 0.5) +   geom_density(aes(x = lambda_chains[, 2]), fill = \"red\", alpha = 0.5) +   labs(     title = \"Density plot of phi chains\",     x = \"Value\",     y = \"Density\",   ) +   theme_minimal() gamma_chains <- cbind(   chain_1$gamma, chain_2$gamma )  ggplot2::ggplot() +   geom_density(aes(x = gamma_chains[, 1]), fill = \"blue\", alpha = 0.5) +   geom_density(aes(x = gamma_chains[, 2]), fill = \"red\", alpha = 0.5) +   labs(     title = \"Density plot of gamma chains\",     x = \"Value\",     y = \"Density\",   ) +   theme_minimal()"},{"path":"https://bjarkehautop.github.io/bayesSSM/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Bjarke Hautop. Author, maintainer.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hautop B (2025). bayesSSM: Bayesian Methods State Space Models. R package version 0.2.5, https://bjarkehautop.github.io/bayesSSM/, https://github.com/BjarkeHautop/bayesSSM.","code":"@Manual{,   title = {bayesSSM: Bayesian Methods for State Space Models},   author = {Bjarke Hautop},   year = {2025},   note = {R package version 0.2.5, https://bjarkehautop.github.io/bayesSSM/},   url = {https://github.com/BjarkeHautop/bayesSSM}, }"},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"bayesssm-","dir":"","previous_headings":"","what":"Bayesian Methods for State Space Models","title":"Bayesian Methods for State Space Models","text":"bayesSSM R package offering set tools performing Bayesian inference state-space models (SSMs). implements Particle Marginal Metropolis-Hastings (PMMH) Bayesian inference.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"why-bayesssm","dir":"","previous_headings":"","what":"Why bayesSSM?","title":"Bayesian Methods for State Space Models","text":"several alternatives available performing particle MCMC, POMP package, designed bayesSSM ease use mind. developed procrastination task Master’s thesis Particle MCMC, since implementing everything scratch anyway. Everything written R, performance best, easy use.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Bayesian Methods for State Space Models","text":"can install development version bayesSSM GitHub :","code":"# install.packages(\"pak\") pak::pak(\"BjarkeHautop/bayesSSM\")"},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Bayesian Methods for State Space Models","text":"Consider following SSM: X1∼N(0,1)Xt=ϕXt−1+sin(Xt−1)+σxVt,Vt∼N(0,1)Yt=Xt+σyWt,Wt∼N(0,1). \\begin{aligned}         X_1 &\\sim N(0,1) \\\\         X_t&=\\phi X_{t-1}+\\sin(X_{t-1})+\\sigma_x V_t, \\quad V_t \\sim N(0,1) \\\\         Y_t&=X_t+\\sigma_y W_t, \\quad W_t \\sim N(0, \\, 1). \\end{aligned} Let’s first simulate data model ϕ=0.8\\phi = 0.8, σx=1\\sigma_x = 1, σy=0.5\\sigma_y = 0.5. define priors model follows: ϕ∼N(0,1),σx∼Exp(1),σy∼Exp(1). \\begin{aligned}         \\phi &\\sim N(0,1), \\\\         \\sigma_x &\\sim \\text{Exp}(1), \\\\         \\sigma_y &\\sim \\text{Exp}(1). \\end{aligned} can use pmmh perform Bayesian inference model. use pmmh need define functions SSM priors. functions init_fn, transition_fn functions simulates latent states. must contain argument particles, vector particles, can contain arguments. function log_likelihood_fn function calculates log-likelihood observed data given latent state variables. must contain arguments y particles. priors parameters must defined log-prior functions. Every parameter init_fn, transition_fn, log_likelihood_fn must corresponding log-prior function. Now can run PMMH algorithm using pmmh function. run 2 chains 200 MCMC samples burn-10. also modify tuning use 200 pilot samples burn-10. practice want run much larger number samples. get convergence warnings ran algorithm small number samples.","code":"t_val <- 20 phi_val <- 0.8 sigma_x_val <- 1 sigma_y_val <- 0.5  x <- numeric(t_val) y <- numeric(t_val) x[1] <- rnorm(1, mean = 0, sd = sigma_x_val) y[1] <- rnorm(1, mean = x[1], sd = sigma_y_val) for (t in 2:t_val) {   x[t] <- phi_val * x[t - 1] + sin(x[t - 1]) + rnorm(1,     mean = 0,     sd = sigma_x_val   )   y[t] <- x[t] + rnorm(1, mean = 0, sd = sigma_y_val) } init_fn <- function(particles) {   stats::rnorm(particles, mean = 0, sd = 1) } transition_fn <- function(particles, phi, sigma_x) {   phi * particles + sin(particles) +     stats::rnorm(length(particles), mean = 0, sd = sigma_x) } log_likelihood_fn <- function(y, particles, sigma_y) {   stats::dnorm(y, mean = particles, sd = sigma_y, log = TRUE) } log_prior_phi <- function(phi) {   stats::dnorm(phi, mean = 0, sd = 1, log = TRUE) } log_prior_sigma_x <- function(sigma) {   stats::dexp(sigma, rate = 1, log = TRUE) } log_prior_sigma_y <- function(sigma) {   stats::dexp(sigma, rate = 1, log = TRUE) }  log_priors <- list(   phi = log_prior_phi,   sigma_x = log_prior_sigma_x,   sigma_y = log_prior_sigma_y ) library(bayesSSM)  result <- pmmh(   y = y,   m = 500, # number of MCMC samples   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   log_priors = log_priors,   init_params = c(phi = 0.5, sigma_x = 0.5, sigma_y = 0.5),   burn_in = 50,   num_chains = 2,   seed = 1405,   tune_control = default_tune_control(pilot_m = 200, pilot_burn_in = 10) ) #> Running chain 1... #> Running pilot chain for tuning... #> Using 100 particles for PMMH: #> Running particle MCMC chain with tuned settings... #> Running chain 2... #> Running pilot chain for tuning... #> Using 100 particles for PMMH: #> Running particle MCMC chain with tuned settings... #> PMMH Results Summary: #>  Parameter Mean   SD Median CI.2.5% CI.97.5% ESS  Rhat #>        phi 0.72 0.11   0.74    0.45     0.92  28 1.007 #>    sigma_x 0.42 0.29   0.41    0.01     1.04  39 1.026 #>    sigma_y 0.84 0.20   0.84    0.44     1.19  39 1.019 #> Warning in pmmh(y = y, m = 500, init_fn = init_fn, transition_fn = #> transition_fn, : Some ESS values are below 400, indicating poor mixing. #> Consider running the chains for more iterations. #> Warning in pmmh(y = y, m = 500, init_fn = init_fn, transition_fn = transition_fn, :  #> Some Rhat values are above 1.01, indicating that the chains have not converged. Consider running the chains for more iterations and/or increase burn_in."},{"path":"https://bjarkehautop.github.io/bayesSSM/index.html","id":"state-space-models","dir":"","previous_headings":"","what":"State-space Models","title":"Bayesian Methods for State Space Models","text":"state-space model (SSM) structure given following directed acyclic graph (DAG):  core function, pmmh, implements Particle Marginal Metropolis-Hastings, algorithm first generates set NN particles approximate likelihood uses approximation acceptance probability. implementation automatically tunes number particles proposal distribution parameters.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ESS.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate effective sample size (ESS) of MCMC chains. — ess","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"Estimate effective sample size (ESS) MCMC chains.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ESS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"","code":"ess(chains)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ESS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"chains matrix dimensions m (iterations) x k (chains).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ESS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"estimated effective sample size (ess) chains.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ESS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"Uses formula ESS proposed Vehtari et al. (2021).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ESS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"Vehtari et al. (2021). Rank-normalization, folding, localization: improved R-hat assessing convergence MCMC. Available : https://doi.org/10.1214/20-BA1221","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/ESS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate effective sample size (ESS) of MCMC chains. — ess","text":"","code":"chains <- matrix(rnorm(3000), nrow = 1000, ncol = 3) ess(chains) #> [1] 2970.728"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/default_tune_control.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Tuning Control Parameters — default_tune_control","title":"Create Tuning Control Parameters — default_tune_control","text":"function creates list tuning parameters used pmmh function. tuning choices inspired Pitt et al. [2012] Dahlin Schön [2019].","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/default_tune_control.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Tuning Control Parameters — default_tune_control","text":"","code":"default_tune_control(   pilot_proposal_sd = 0.5,   pilot_n = 100,   pilot_m = 2000,   pilot_target_var = 1,   pilot_burn_in = 500,   pilot_reps = 100,   pilot_algorithm = c(\"SISAR\", \"SISR\", \"SIS\"),   pilot_resample_fn = c(\"stratified\", \"systematic\", \"multinomial\") )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/default_tune_control.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Tuning Control Parameters — default_tune_control","text":"pilot_proposal_sd Standard deviation pilot proposals. Default 0.5. pilot_n Number pilot particles particle filter. Default 100. pilot_m Number iterations MCMC. Default 2000. pilot_target_var target variance posterior log-likelihood evaluated estimated posterior mean. Default 1. pilot_burn_in Number burn-iterations MCMC. Default 500. pilot_reps Number times particle filter run. Default 100. pilot_algorithm algorithm used pilot particle filter. Default \"SISAR\". pilot_resample_fn resampling function used pilot particle filter. Default \"stratified\".","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/default_tune_control.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Tuning Control Parameters — default_tune_control","text":"list tuning control parameters.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/default_tune_control.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create Tuning Control Parameters — default_tune_control","text":"M. K. Pitt, R. d. S. Silva, P. Giordani, R. Kohn. properties Markov chain Monte Carlo simulation methods based particle filter. Journal Econometrics, 171(2):134–151, 2012. doi: https://doi.org/10.1016/j.jeconom.2012.06.004 J. Dahlin T. B. Schön. Getting started particle Metropolis-Hastings inference nonlinear dynamical models. Journal Statistical Software, 88(2):1–41, 2019. doi: 10.18637/jss.v088.c02","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-back_transform_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to back-transform parameters — .back_transform_params","title":"Internal function to back-transform parameters — .back_transform_params","text":"Internal function back-transform parameters","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-back_transform_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to back-transform parameters — .back_transform_params","text":"","code":".back_transform_params(theta_trans, transform)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-back_transform_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function to back-transform parameters — .back_transform_params","text":"theta_trans transformed parameter vector transform transformation type parameter","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-back_transform_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function to back-transform parameters — .back_transform_params","text":"original parameter vector","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-check_params_match.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper function to validate input of user-defined functions and priors — .check_params_match","title":"Helper function to validate input of user-defined functions and priors — .check_params_match","text":"Helper function validate input user-defined functions priors","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-check_params_match.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper function to validate input of user-defined functions and priors — .check_params_match","text":"","code":".check_params_match(   init_fn,   transition_fn,   log_likelihood_fn,   init_params,   log_priors )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-check_params_match.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper function to validate input of user-defined functions and priors — .check_params_match","text":"init_fn function initialize state-space model. transition_fn function defines state transition state-space model. log_likelihood_fn function calculates log-likelihood state-space model given latent states. init_params vector initial parameter values. log_priors list functions computing log-prior parameter.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-compute_log_jacobian.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to compute the Jacobian of the transformation — .compute_log_jacobian","title":"Internal function to compute the Jacobian of the transformation — .compute_log_jacobian","text":"Internal function compute Jacobian transformation","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-compute_log_jacobian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to compute the Jacobian of the transformation — .compute_log_jacobian","text":"","code":".compute_log_jacobian(theta, transform)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-compute_log_jacobian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function to compute the Jacobian of the transformation — .compute_log_jacobian","text":"theta parameter vector (original scale) transform transformation type parameter","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-compute_log_jacobian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function to compute the Jacobian of the transformation — .compute_log_jacobian","text":"log-Jacobian transformation","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-pilot_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Pilot Run for Particle Filter Tuning — .pilot_run","title":"Pilot Run for Particle Filter Tuning — .pilot_run","text":"internal function repeatedly evaluates particle filter order estimate variance log-likelihoods compute recommended target number particles Particle Marginal Metropolis Hastings (PMMH) algorithm.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-pilot_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pilot Run for Particle Filter Tuning — .pilot_run","text":"","code":".pilot_run(   y,   pilot_n,   pilot_reps,   init_fn,   transition_fn,   log_likelihood_fn,   obs_times = NULL,   algorithm = c(\"SISAR\", \"SISR\", \"SIS\"),   resample_fn = c(\"stratified\", \"systematic\", \"multinomial\"),   ... )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-pilot_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pilot Run for Particle Filter Tuning — .pilot_run","text":"y numeric vector matrix observations. row represents observation time step. pilot_n integer specifying initial number particles use. pilot_reps integer specifying number repetitions pilot run. init_fn function initializes particle states. take current particles first argument return vector matrix initial particle states. transition_fn function describing state transition model. take current particles current time step arguments return propagated particles. log_likelihood_fn function computes log likelihoods particles. accept observation, current particles, current time step arguments return numeric vector log likelihood values. obs_times numeric vector observation times. provided, function assumes observations available every time step. algorithm character string specifying particle filtering algorithm use. Must one \"SISAR\", \"SISR\", \"SIS\". Defaults \"SISAR\". resample_fn character string specifying resampling method. Must one \"stratified\", \"systematic\", \"multinomial\". Defaults \"stratified\". ... Additional arguments passed `init_fn`, `transition_fn`, `log_likelihood_fn`.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-pilot_run.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pilot Run for Particle Filter Tuning — .pilot_run","text":"list containing: variance_estimate estimated variance log-likelihoods   pilot run. target_N number particles used PMMH algorithm. pilot_loglikes numeric vector log-likelihood values computed   run.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-pilot_run.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pilot Run for Particle Filter Tuning — .pilot_run","text":"function performs pilot_reps evaluations particle filter using provided parameter vector theta. estimates variance log-likelihoods scales initial particle number variance. final number particles taken ceiling scaled value minimum 100 maximum 1000.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-resample_multinomial.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal Resampling Functions — .resample_multinomial","title":"Internal Resampling Functions — .resample_multinomial","text":"Helper functions resampling particles particle filter. functions implement multinomial, stratified, systematic resampling.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-resample_multinomial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal Resampling Functions — .resample_multinomial","text":"","code":".resample_multinomial(particles, weights)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-run_pilot_chain.html","id":null,"dir":"Reference","previous_headings":"","what":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","title":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","text":"Run Pilot Chain Posterior Estimation","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-run_pilot_chain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","text":"","code":".run_pilot_chain(   y,   pilot_m,   pilot_n,   pilot_reps,   init_fn,   transition_fn,   log_likelihood_fn,   log_priors,   proposal_sd,   obs_times = NULL,   algorithm = c(\"SISAR\", \"SISR\", \"SIS\"),   resample_fn = c(\"stratified\", \"systematic\", \"multinomial\"),   param_transform = NULL,   init_params = NULL,   verbose = FALSE,   ... )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-run_pilot_chain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","text":"y numeric vector observations. pilot_m integer specifying number iterations pilot chain. pilot_n integer specifying number particles particle filter. pilot_reps integer specifying number repetitions pilot run. init_fn function initialize state space model. transition_fn function defines state transition dynamics state space model. log_likelihood_fn function computes log-likelihood observations given model parameters. log_priors list functions representing log-priors model parameter. proposal_sd numeric vector specifying standard deviations random walk proposal distribution parameter. obs_times numeric vector observation times. provided, function assumes observations available every time step. algorithm character string specifying particle filter algorithm use. One \"SISAR\", \"SISR\", \"SIS\". Default \"SISAR\". resample_fn character string specifying resampling method use. One \"stratified\", \"systematic\", \"multinomial\". Default \"stratified\". param_transform character vector specifying parameter transformations proposing parameters using random walk. Currently supports \"log\" log-transformation \"identity\" transformation. Default `NULL`, correspond transformation (\"identity). init_params numeric vector initial parameter values. `NULL`, default vector ones. Default `NULL`. ... Additional arguments passed particle filter function.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-run_pilot_chain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","text":"list containing: pilot_theta_mean numeric vector posterior mean parameters. pilot_theta_cov matrix posterior covariance (variance one parameter). target_N estimated target number particles PMMH algorithm. pilot_theta_chain matrix containing chain parameter values throughout pilot run. pilot_loglike_chain vector containing log-likelihood values associated iteration pilot chain.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-run_pilot_chain.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run Pilot Chain for Posterior Estimation — .run_pilot_chain","text":"function runs pilot chain estimate posterior mean covariance model parameters using particle filter. chain run `pilot_m` iterations, iteration proposing new parameters evaluating likelihood prior. chain used estimate posterior mean covariance, used tune number particles Particle Marginal Metropolis Hastings (PMMH) algorithm.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-transform_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to transform parameters — .transform_params","title":"Internal function to transform parameters — .transform_params","text":"Internal function transform parameters","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-transform_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to transform parameters — .transform_params","text":"","code":".transform_params(theta, transform)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-transform_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function to transform parameters — .transform_params","text":"theta parameter vector transform transformation type parameter","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/dot-transform_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function to transform parameters — .transform_params","text":"transformed parameter vector","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter.html","id":null,"dir":"Reference","previous_headings":"","what":"Particle Filter — particle_filter","title":"Particle Filter — particle_filter","text":"function implements bootstrap particle filter estimating hidden states state space model using sequential Monte Carlo methods. Three filtering variants supported: SIS: Sequential Importance Sampling (without resampling). SISR: Sequential Importance Sampling resampling   every time step. SISAR: SIS adaptive resampling based Effective   Sample Size (ESS). Resampling triggered ESS falls   given threshold (default particles / 2). recommended use either SISR SISAR avoid weight degeneracy.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Particle Filter — particle_filter","text":"","code":"particle_filter(   y,   num_particles,   init_fn,   transition_fn,   log_likelihood_fn,   obs_times = NULL,   algorithm = c(\"SISAR\", \"SISR\", \"SIS\"),   resample_fn = c(\"stratified\", \"systematic\", \"multinomial\"),   threshold = NULL,   return_particles = TRUE,   ... )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Particle Filter — particle_filter","text":"y numeric vector matrix observations. row represents observation time step. num_particles positive integer specifying number particles. init_fn function initializes particle states. take current particles first argument return vector matrix initial particle states. transition_fn function describing state transition model. take current particles current time step arguments return propagated particles. log_likelihood_fn function computes log likelihoods particles. accept observation, current particles, current time step arguments return numeric vector log likelihood values. obs_times numeric vector observation times. provided, function assumes observations available every time step. algorithm character string specifying particle filtering algorithm use. Must one \"SISAR\", \"SISR\", \"SIS\". Defaults \"SISAR\". resample_fn character string specifying resampling method. Must one \"stratified\", \"systematic\", \"multinomial\". Defaults \"stratified\". threshold numeric value specifying ESS threshold triggering resampling \"SISAR\" algorithm. provided, defaults particles / 2. return_particles logical value indicating whether return full particle history. Defaults TRUE. ... Additional arguments passed init_fn, transition_fn, log_likelihood_fn.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Particle Filter — particle_filter","text":"list containing: state_est numeric vector estimated states     time, computed weighted average particles. ess numeric vector Effective Sample Size (ESS)     time step. loglike accumulated log-likelihood observations given     model. loglike_history numeric vector log-likelihood     time step. algorithm character string indicating filtering algorithm     used. particles_history (Optional) list particle state matrices     time (one per time step), returned return_particles     TRUE. weights_history (Optional) list particle weight vectors     time (one per time step), returned return_particles     TRUE.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Particle Filter — particle_filter","text":"particle filter sequential Monte Carlo method approximates posterior distribution state state space model. three supported algorithms differ approach resampling: SIS: Particles propagated weighted without    resampling, may lead weight degeneracy time. SISR: Resampling performed every time step combat   weight degeneracy. SISAR: Resampling performed adaptively; particles   resampled Effective Sample Size (ESS) falls   specified threshold (defaulting particles / 2). Effective Sample Size (ESS) context particle filters defined $$ESS = \\left(\\sum_{=1}^{\\text{n}} w_i^2\\right)^{-1},$$ \\(n\\) number particles \\(w_i\\) normalized weights particles. default resampling method stratified resampling, Douc et al., 2005 showed always gives lower variance compared multinomial resampling.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Particle Filter — particle_filter","text":"Douc, R., Cappé, O., & Moulines, E. (2005). Comparison Resampling Schemes Particle Filtering. Accessible : https://arxiv.org/abs/cs/0507025","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/particle_filter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Particle Filter — particle_filter","text":"","code":"init_fn <- function(particles) rnorm(particles, 0, 1) transition_fn <- function(particles) particles + rnorm(length(particles)) log_likelihood_fn <- function(y, particles) {   dnorm(y, mean = particles, sd = 1, log = TRUE) }  # Generate data y <- cumsum(rnorm(50)) num_particles <- 100  # Run the particle filter using default settings. result <- particle_filter(   y = y,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn ) plot(result$state_est, type = \"l\", col = \"blue\", main = \"State Estimates\")   # With parameters init_fn <- function(particles) rnorm(particles, 0, 1) transition_fn <- function(particles, mu) {   particles + rnorm(length(particles), mean = mu) } log_likelihood_fn <- function(y, particles, sigma) {   dnorm(y, mean = particles, sd = sigma, log = TRUE) }  # Generate data y <- cumsum(rnorm(50)) num_particles <- 100  # Run the particle filter using default settings. result <- particle_filter(   y = y,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   mu = 1,   sigma = 1 ) plot(result$state_est, type = \"l\", col = \"blue\", main = \"State Estimates\")   # With observations gaps init_fn <- function(particles) rnorm(particles, 0, 1) transition_fn <- function(particles, mu) {   particles + rnorm(length(particles), mean = mu) } log_likelihood_fn <- function(y, particles, sigma) {   dnorm(y, mean = particles, sd = sigma, log = TRUE) }  # Generate data using DGP simulate_ssm <- function(num_steps, mu, sigma) {   x <- numeric(num_steps)   y <- numeric(num_steps)   x[1] <- rnorm(1, mean = 0, sd = sigma)   y[1] <- rnorm(1, mean = x[1], sd = sigma)   for (t in 2:num_steps) {     x[t] <- mu * x[t - 1] + sin(x[t - 1]) + rnorm(1, mean = 0, sd = sigma)     y[t] <- x[t] + rnorm(1, mean = 0, sd = sigma)   }   y }  data <- simulate_ssm(10, mu = 1, sigma = 1) # Suppose we have data for t=1,2,3,5,6,7,8,9,10 (i.e., missing at t=4)  obs_times <- c(1, 2, 3, 5, 6, 7, 8, 9, 10) data <- data[obs_times]  num_particles <- 100 # Run the particle filter # Specify observation times in the particle filter using obs_times result <- particle_filter(   y = data,   num_particles = num_particles,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   obs_times = obs_times,   mu = 1,   sigma = 1, ) plot(result$state_est, type = \"l\", col = \"blue\", main = \"State Estimates\")"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":null,"dir":"Reference","previous_headings":"","what":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"function implements Particle Marginal Metropolis-Hastings (PMMH) algorithm perform Bayesian inference state-space models. first runs pilot chain tune proposal distribution number particles particle filter, runs main PMMH chain.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"","code":"pmmh(   y,   m,   init_fn,   transition_fn,   log_likelihood_fn,   log_priors,   init_params,   burn_in,   num_chains = 4,   obs_times = NULL,   algorithm = c(\"SISAR\", \"SISR\", \"SIS\"),   resample_fn = c(\"stratified\", \"systematic\", \"multinomial\"),   param_transform = NULL,   tune_control = default_tune_control(),   verbose = FALSE,   return_latent_state_est = FALSE,   seed = NULL,   num_cores = 1 )"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"y numeric vector matrix observations. row represents observation time step. m integer specifying total number MCMC iterations. init_fn function initialize state-space model. transition_fn function defines state transition state-space model. log_likelihood_fn function calculates log-likelihood state-space model given latent states. log_priors list functions computing log-prior parameter. init_params vector initial parameter values. burn_in integer indicating number initial MCMC iterations discard burn-. num_chains integer specifying number PMMH chains run. obs_times numeric vector observation times. provided, function assumes observations available every time step. algorithm character string specifying particle filtering algorithm use. Must one \"SISAR\", \"SISR\", \"SIS\". Defaults \"SISAR\". resample_fn character string specifying resampling method. Must one \"stratified\", \"systematic\", \"multinomial\". Defaults \"stratified\". param_transform optional character vector specifies transformation applied parameter. Currently supports \"log\", \"logit\", \"identity\". NULL, \"identity\" transformation used parameters. tune_control list generated default_tune_control containing tuning parameters pilot chain, pilot_m, pilot_n, pilot_reps, pilot_proposal_sd, pilot_algorithm, pilot_resample_fn. verbose logical value indicating whether print information pilot_run tuning. Defaults FALSE. return_latent_state_est logical value indicating whether return latent state estimates time step. Defaults FALSE. seed optional integer set seed reproducibility. num_cores integer specifying number cores use parallel processing. Defaults 1. chain assigned core, number cores exceed number chains (num_chains). progress information given user limited using one core.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"list containing: theta_chain matrix post burn-parameter samples. latent_state_chain return_latent_state_est   TRUE, list matrices containing latent state estimates   time step. diagnostics Diagnostics containing ESS Rhat   parameter.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"PMMH algorithm essentially Metropolis Hastings algorithm instead using exact likelihood estimated using particle filter (see also particle_filter). implementation two main steps: Pilot Chain: pilot particle chain run using   settings provided tune_control obtain initial estimates   parameter vector, covariance, number particles   Particle Filter. Main MCMC Chain: main PMMH chain executed   m iterations using tuned settings.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"Andrieu et al. (2010). Particle Markov chain Monte Carlo methods. Journal Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342. doi: 10.1111/j.1467-9868.2009.00736.x","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/pmmh.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Particle Marginal Metropolis-Hastings (PMMH) for State-Space Models — pmmh","text":"","code":"init_fn <- function(particles) {   rnorm(particles, mean = 0, sd = 1) } transition_fn <- function(particles, phi, sigma_x) {   phi * particles + sin(particles) +     rnorm(length(particles), mean = 0, sd = sigma_x) } log_likelihood_fn <- function(y, particles, sigma_y) {   dnorm(y, mean = particles, sd = sigma_y, log = TRUE) } log_prior_phi <- function(phi) {   dnorm(phi, mean = 0, sd = 1, log = TRUE) } log_prior_sigma_x <- function(sigma) {   dexp(sigma, rate = 1, log = TRUE) } log_prior_sigma_y <- function(sigma) {   dexp(sigma, rate = 1, log = TRUE) } log_priors <- list(   phi = log_prior_phi,   sigma_x = log_prior_sigma_x,   sigma_y = log_prior_sigma_y ) # Generate data t_val <- 20 x <- numeric(t_val) y <- numeric(t_val) x[1] <- rnorm(1, mean = 0, sd = 1) y[1] <- rnorm(1, mean = x[1], sd = 0.5) for (t in 2:t_val) {   x[t] <- 0.8 * x[t - 1] + sin(x[t - 1]) + rnorm(1, mean = 0, sd = 1)   y[t] <- x[t] + rnorm(1, mean = 0, sd = 0.5) } # Should use much higher MCMC iterations in practice (m) pmmh_result <- pmmh(   y = y,   m = 1000,   init_fn = init_fn,   transition_fn = transition_fn,   log_likelihood_fn = log_likelihood_fn,   log_priors = log_priors,   init_params = c(phi = 0.8, sigma_x = 1, sigma_y = 0.5),   burn_in = 100,   num_chains = 2,   param_transform = list(     phi = \"identity\",     sigma_x = \"log\",     sigma_y = \"log\"   ),   tune_control = default_tune_control(pilot_m = 500, pilot_burn_in = 100) ) #> Running chain 1... #> Running pilot chain for tuning... #> Using 100 particles for PMMH: #> Running particle MCMC chain with tuned settings... #> Running chain 2... #> Running pilot chain for tuning... #> Using 100 particles for PMMH: #> Running particle MCMC chain with tuned settings... #> PMMH Results Summary: #>  Parameter Mean   SD Median CI.2.5% CI.97.5% ESS  Rhat #>        phi 0.87 0.08   0.88    0.70     1.00 166 1.023 #>    sigma_x 0.55 0.35   0.55    0.01     1.26  32 1.002 #>    sigma_y 0.73 0.30   0.77    0.14     1.25  26 1.011 #> Warning: Some ESS values are below 400, indicating poor mixing. Consider running the chains for more iterations. #> Warning:  #> Some Rhat values are above 1.01, indicating that the chains have not converged. Consider running the chains for more iterations and/or increase burn_in. # Convergence warning is expected with such low MCMC iterations."},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/print.pmmh_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for PMMH output — print.pmmh_output","title":"Print method for PMMH output — print.pmmh_output","text":"Print method PMMH output","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/print.pmmh_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for PMMH output — print.pmmh_output","text":"","code":"# S3 method for class 'pmmh_output' print(x, ...)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/print.pmmh_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for PMMH output — print.pmmh_output","text":"x object class `pmmh_output`. ... Additional arguments.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/print.pmmh_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for PMMH output — print.pmmh_output","text":"object `x` invisibly.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/print.pmmh_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print method for PMMH output — print.pmmh_output","text":"","code":"# Create dummy chains for two parameters across two chains set.seed(1405) chain1 <- data.frame(param1 = rnorm(100), param2 = rnorm(100)) chain2 <- data.frame(param1 = rnorm(100), param2 = rnorm(100)) dummy_output <- list(   theta_chain = list(chain1, chain2),   diagnostics = list(     ess = c(param1 = 200, param2 = 190),     rhat = c(param1 = 1.01, param2 = 1.00)   ) ) class(dummy_output) <- \"pmmh_output\" print(dummy_output) #> PMMH Results Summary: #>  Parameter  Mean   SD Median CI.2.5% CI.97.5% ESS Rhat #>     param1 -0.02 0.94   0.04   -1.85     1.71 200 1.01 #>     param2  0.03 1.01   0.00   -1.84     2.40 190 1.00"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute split Rhat statistic — rhat","title":"Compute split Rhat statistic — rhat","text":"Compute split Rhat statistic","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute split Rhat statistic — rhat","text":"","code":"rhat(chains)"},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute split Rhat statistic — rhat","text":"chains matrix dimensions m (iterations) x k (chains).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute split Rhat statistic — rhat","text":"split-Rhat statistic.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute split Rhat statistic — rhat","text":"Uses formula split-Rhat proposed Gelman et al. (2013).","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute split Rhat statistic — rhat","text":"Gelman et al. (2013). Bayesian Data Analysis, 3rd Edition.","code":""},{"path":"https://bjarkehautop.github.io/bayesSSM/reference/rhat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute split Rhat statistic — rhat","text":"","code":"chains <- matrix(rnorm(3000), nrow = 1000, ncol = 3) rhat(chains) #> [1] 1"}]
